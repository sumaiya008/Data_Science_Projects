{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a208f288",
   "metadata": {},
   "source": [
    "https://campus.datacamp.com/courses/customer-analytics-and-ab-testing-in-python/key-performance-indicators-measuring-business-success?ex=5\n",
    "\n",
    "#### Loading & examining our data\n",
    "Let's begin by loading and examining two datasets: one that contains a set of user demographics and the other -- a set of data relating to in-app purchases for our meditation app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1df44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas \n",
    "import pandas as pd\n",
    "\n",
    "# Load the customer_data\n",
    "customer_data = pd.read_csv('customer_data.csv')\n",
    "\n",
    "# Load the app_purchases\n",
    "app_purchases = pd.read_csv('inapp_purchases.csv')\n",
    "\n",
    "# Print the columns of customer data\n",
    "print(customer_data.columns)\n",
    "\n",
    "# Print the columns of app_purchases\n",
    "print(app_purchases.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6361566c",
   "metadata": {},
   "source": [
    "##### Merging on different sets of fields\n",
    "As you saw in the previous exercise, both customer_data and app_purchases have a common 'uid' column that you can use to combine them. If you explored them further, you would discover that they also have a common date column that is named 'date' in app_purchases and 'reg_date' in customer_data.\n",
    "\n",
    "In this exercise you will explore merging on both of these columns and looking at how this impacts your final results.\n",
    "\n",
    "The two datasets from the previous exercise - customer_data and app_purchases- have been loaded for you, with 'reg_date' in customer_data renamed to 'date'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af13845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on the 'uid' field\n",
    "uid_combined_data = app_purchases.merge(customer_data, on=['uid'], how='inner')\n",
    "\n",
    "# Examine the results \n",
    "print(uid_combined_data.head())\n",
    "print(len(uid_combined_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4c12f",
   "metadata": {},
   "source": [
    "#### Practicing aggregations\n",
    "It's time to begin exploring the in-app purchase data in more detail. Here, you will practice aggregating the dataset in various ways using the .agg() method and then examine the results to get an understanding of the overall data, as well as a feel for how to aggregate data using pandas.\n",
    "\n",
    "Loaded for you is a DataFrame named purchase_data which is the dataset of in-app purchase data merged with the user demographics data from earlier.\n",
    "\n",
    "Before getting started, it's good practice to explore this purchase_data DataFrame in the IPython Shell. In particular, notice the price column: you'll examine it further in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d8a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean purchase price \n",
    "purchase_price_mean = purchase_data.price.agg('mean')\n",
    "\n",
    "# Examine the output \n",
    "print(purchase_price_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8253fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and median purchase price \n",
    "purchase_price_summary = purchase_data.price.agg(['mean', 'median'])\n",
    "\n",
    "# Examine the output \n",
    "print(purchase_price_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf7abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and median of price and age\n",
    "purchase_summary = purchase_data.agg({'price': ['mean', 'median'], 'age': ['mean', 'median']})\n",
    "\n",
    "# Examine the output \n",
    "print(purchase_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb9d035",
   "metadata": {},
   "source": [
    "#### Grouping & aggregating\n",
    "You'll be using .groupby() and .agg() a lot in this course, so it's important to become comfortable with them. In this exercise, your job is to calculate a set of summary statistics about the purchase data broken out by 'device' (Android or iOS) and 'gender' (Male or Female).\n",
    "\n",
    "Following this, you'll compare the values across these subsets, which will give you a baseline for these values as potential KPIs to optimize going forward.\n",
    "\n",
    "The purchase_data DataFrame from the previous exercise has been pre-loaded for you. As a reminder, it contains purchases merged with user demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f2731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data \n",
    "grouped_purchase_data = purchase_data.groupby(by = ['device', 'gender'])\n",
    "\n",
    "# Aggregate the data\n",
    "purchase_summary = grouped_purchase_data.agg({'price': ['mean', 'median', 'std']})\n",
    "\n",
    "# Examine the results\n",
    "print(purchase_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7853ce",
   "metadata": {},
   "source": [
    "#### Calculating KPIs\n",
    "You're now going to take what you've learned and work through calculating a KPI yourself. Specifically, you'll calculate the average amount paid per purchase within a user's first 28 days using the purchase_data DataFrame from before.\n",
    "\n",
    "This KPI can provide a sense of the popularity of different in-app purchase price points to users within their first month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66519648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute max_purchase_date\n",
    "max_purchase_date = current_date - timedelta(days=28)\n",
    "\n",
    "# Filter to only include users who registered before our max date\n",
    "purchase_data_filt = purchase_data[purchase_data.reg_date < max_purchase_date]\n",
    "\n",
    "# Filter to contain only purchases within the first 28 days of registration\n",
    "purchase_data_filt = purchase_data_filt[(purchase_data_filt.date <= \n",
    "                        purchase_data_filt.reg_date + timedelta(days=28))]\n",
    "\n",
    "# Output the mean price paid per purchase\n",
    "print(purchase_data_filt.price.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6a7f6a",
   "metadata": {},
   "source": [
    "#### Average purchase price by cohort\n",
    "Building on the previous exercise, let's look at the same KPI, average purchase price, and a similar one, median purchase price, within the first 28 days. Additionally, let's look at these metrics not limited to 28 days to compare.\n",
    "\n",
    "We can calculate these metrics across a set of cohorts and see what differences emerge. This is a useful task as it can help us understand how behaviors vary across cohorts.\n",
    "\n",
    "Note that in our data the price variable is given in cents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0620029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the max registration date to be one month before today\n",
    "max_reg_date = current_date - timedelta(days=28)\n",
    "\n",
    "# Find the month 1 values\n",
    "month1 = np.where((purchase_data.reg_date < max_reg_date) &\n",
    "                 (purchase_data.date < purchase_data.reg_date + timedelta(days=28)),\n",
    "                  purchase_data.price, \n",
    "                  np.NaN)\n",
    "                 \n",
    "# Update the value in the DataFrame\n",
    "purchase_data['month1'] = month1\n",
    "\n",
    "# Group the data by gender and device \n",
    "purchase_data_upd = purchase_data.groupby(by=['gender', 'device'], as_index=False) \n",
    "\n",
    "# Aggregate the month1 and price data \n",
    "purchase_summary = purchase_data_upd.agg(\n",
    "                        {'month1': ['mean', 'median'],\n",
    "                        'price': ['mean', 'median']})\n",
    "\n",
    "# Examine the results \n",
    "print(purchase_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc791ba",
   "metadata": {},
   "source": [
    "#### Parsing dates\n",
    "In this exercise you will practice parsing dates in Python. While often data pulled from a database will be correctly formatted, other data sources can be less nice. Knowing how to properly parse dates is crucial to get the data in a workable format. For reference refer to https://strftime.org/ throughout this exercise to see date format to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the correct format for the date\n",
    "date_data_one = pd.to_datetime(date_data_one, format='%A %B %d, %Y')\n",
    "print(date_data_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec40dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the correct format for the date\n",
    "date_data_two = pd.to_datetime(date_data_two, format='%Y-%m-%d')\n",
    "print(date_data_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c71673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the correct format for the date\n",
    "date_data_three = pd.to_datetime(date_data_three, format='%m/%d/%Y')\n",
    "print(date_data_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4386e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the correct format for the date\n",
    "date_data_four = pd.to_datetime(date_data_four, format='%Y %B %d %H:%M')\n",
    "print(date_data_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdb63fa",
   "metadata": {},
   "source": [
    "#### Plotting time series data\n",
    "In trying to boost purchases, we have made some changes to our introductory in-app purchase pricing. In this exercise, you will check if this is having an impact on the number of purchases made by purchasing users during their first week.\n",
    "\n",
    "The dataset user_purchases has been joined to the demographics data and properly filtered. The column 'first_week_purchases' that is 1 for a first week purchase and 0 otherwise has been added. This column is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f455403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data and aggregate first_week_purchases\n",
    "user_purchases = user_purchases.groupby(by=['reg_date', 'uid']).agg({'first_week_purchases': ['sum']})\n",
    "\n",
    "# Reset the indexes\n",
    "user_purchases.columns = user_purchases.columns.droplevel(level=1)\n",
    "user_purchases.reset_index(inplace=True)\n",
    "\n",
    "# Find the average number of purchases per day by first-week users\n",
    "user_purchases = user_purchases.groupby(by=['reg_date']).agg({'first_week_purchases': ['mean']})\n",
    "user_purchases.columns = user_purchases.columns.droplevel(level=1)\n",
    "user_purchases.reset_index(inplace=True)\n",
    "\n",
    "# Plot the results\n",
    "user_purchases.plot(x='reg_date', y='first_week_purchases')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2046a197",
   "metadata": {},
   "source": [
    "#### Pivoting our data\n",
    "As you saw, there does seem to be an increase in the number of purchases by purchasing users within their first week. Let's now confirm that this is not driven only by one segment of users. We'll do this by first pivoting our data by 'country' and then by 'device'. Our change is designed to impact all of these groups equally.\n",
    "\n",
    "The user_purchases data from before has been grouped and aggregated by the 'country' and 'device' columns. These objects are available in your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972e2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data\n",
    "country_pivot = pd.pivot_table(user_purchases_country, values=['first_week_purchases'], columns=['country'], index=['reg_date'])\n",
    "print(country_pivot.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df65acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data\n",
    "device_pivot = pd.pivot_table(user_purchases_device, values=['first_week_purchases'], columns=['device'], index=['reg_date'])\n",
    "print(device_pivot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e9682",
   "metadata": {},
   "source": [
    "#### Examining the different cohorts\n",
    "To finish this lesson, you're now going to plot by 'country' and then by 'device' and examine the results. Hopefully you will see the observed lift across all groups as designed. This would point to the change being the cause of the lift, not some other event impacting the purchase rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37781a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average first week purchases for each country by registration date\n",
    "country_pivot.plot(x='reg_date', y=['USA', 'CAN', 'FRA', 'BRA', 'TUR', 'DEU'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb70c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average first week purchases for each device by registration date\n",
    "device_pivot.plot(x='reg_date', y=['and', 'iOS'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cea873",
   "metadata": {},
   "source": [
    "#### Seasonality and moving averages\n",
    "Stepping back, we will now look at the overall revenue data for our meditation app. We saw strong purchase growth in one of our products, and now we want to see if that is leading to a corresponding rise in revenue. As you may expect, revenue is very seasonal, so we want to correct for that and unlock macro trends.\n",
    "\n",
    "In this exercise, we will correct for weekly, monthly, and yearly seasonality and plot these over our raw data. This can reveal trends in a very"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fcde8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute 7_day_rev\n",
    "daily_revenue['7_day_rev'] = daily_revenue.revenue.rolling(window=7,center=False).mean()\n",
    "\n",
    "# Compute 28_day_rev\n",
    "daily_revenue['28_day_rev'] = daily_revenue.revenue.rolling(window=28,center=False).mean()\n",
    "    \n",
    "# Compute 365_day_rev\n",
    "daily_revenue['365_day_rev'] = daily_revenue.revenue.rolling(window=365,center=False).mean()\n",
    "    \n",
    "# Plot date, and revenue, along with the 3 rolling functions (in order)    \n",
    "daily_revenue.plot(x='date', y=['revenue', '7_day_rev', '28_day_rev', '365_day_rev', ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba758111",
   "metadata": {},
   "source": [
    "#### Exponential rolling average & over/under smoothing\n",
    "In the previous exercise, we saw that our revenue is somewhat flat over time. In this exercise we will dive deeper into the data to see if we can determine why this is the case. We will look at the revenue for a single in-app purchase product we are selling to see if this potentially reveals any trends. As this will have less data then looking at our overall revenue it will be much noisier. To account for this we will smooth the data using an exponential rolling average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94492525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 'small_scale'\n",
    "daily_revenue['small_scale'] = daily_revenue.revenue.ewm(span=10).mean()\n",
    "\n",
    "# Calculate 'medium_scale'\n",
    "daily_revenue['medium_scale'] = daily_revenue.revenue.ewm(span=100).mean()\n",
    "\n",
    "# Calculate 'large_scale'\n",
    "daily_revenue['large_scale'] = daily_revenue.revenue.ewm(span=500).mean()\n",
    "\n",
    "# Plot 'date' on the x-axis and, our three averages and 'revenue'\n",
    "# on the y-axis\n",
    "daily_revenue.plot(x = 'date', y =['revenue', 'small_scale', 'medium_scale', 'large_scale'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e300043",
   "metadata": {},
   "source": [
    "#### Visualizing user spending\n",
    "Recently, the Product team made some big changes to both the Android & iOS apps. They do not have any direct concerns about the impact of these changes, but want you to monitor the data to make sure that the changes don't hurt company revenue. Additionally, the product team believes that some of these changes may impact female users more than male users.\n",
    "\n",
    "In this exercise you're going to plot the monthly revenue for one of the updated products and evaluate the results.The dataset user_revenue containing the 'device', 'gender', 'country', 'date', and 'revenue' has been loaded. It has been grouped by month, device, and gender. Note that here, a 'month' column has been extracted from the 'date' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3197ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot user_revenue\n",
    "pivoted_data = pd.pivot_table(user_revenue, values ='revenue', columns=['device', 'gender'], index='month')\n",
    "pivoted_data = pivoted_data[1:(len(pivoted_data) -1 )]\n",
    "\n",
    "# Create and show the plot\n",
    "pivoted_data.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e15f741",
   "metadata": {},
   "source": [
    "#### Experimental units: Revenue per user day\n",
    "We are going to check what happens when we add a consumable paywall to our app. A paywall is a feature of a website or other technology that requires payment from users in order to access additional content or services.\n",
    "\n",
    "Here, you'll practice calculating experimental units and baseline values related to our consumable paywall. Both measure revenue only among users who viewed a paywall. Your job is to calculate revenue per user-day, with user-day as the experimental unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a052cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'day'; value from the timestamp\n",
    "purchase_data.date = purchase_data.date.dt.floor('d')\n",
    "\n",
    "# Replace the NaN price values with 0 \n",
    "purchase_data.price = np.where(np.isnan(purchase_data.price), 0, purchase_data.price)\n",
    "\n",
    "# Aggregate the data by 'uid' & 'date'\n",
    "purchase_data_agg = purchase_data.groupby(by=['uid', 'date'], as_index=False)\n",
    "revenue_user_day = purchase_data_agg.sum()\n",
    "\n",
    "# Calculate the final average\n",
    "revenue_user_day = revenue_user_day.price.mean()\n",
    "print(revenue_user_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912b5904",
   "metadata": {},
   "source": [
    "##### Conversion rate sensitivities\n",
    "To mix things up, we will spend the next few exercises working with the conversion rate metric we explored in Chapter One. Specifically you will work to examine what that value becomes under different percentage lifts and look at how many more conversions per day this change would result in. First you will find the average number of paywall views and purchases that were made per day in our observed sample. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f3af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and group the datasets\n",
    "purchase_data = demographics_data.merge(paywall_views,  how='inner', on=['uid'])\n",
    "purchase_data.date = purchase_data.date.dt.floor('d')\n",
    "\n",
    "# Group and aggregate our combined dataset \n",
    "daily_purchase_data = purchase_data.groupby(by=['date'], as_index=False)\n",
    "daily_purchase_data = daily_purchase_data.agg({'purchase': ['sum', 'count']})\n",
    "\n",
    "# Find the mean of each field and then multiply by 1000 to scale the result\n",
    "daily_purchases = daily_purchase_data.purchase['sum'].mean()\n",
    "daily_paywall_views = daily_purchase_data.purchase['count'].mean()\n",
    "daily_purchases = daily_purchases * 1000\n",
    "daily_paywall_views = daily_paywall_views * 1000\n",
    "\n",
    "print(daily_purchases)\n",
    "print(daily_paywall_views)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92efe4b1",
   "metadata": {},
   "source": [
    "#### Sensitivity\n",
    "Continuing with the conversion rate metric, you will now utilize the results from the previous exercise to evaluate a few potential sensitivities that we could make use of in planning our experiment. The baseline conversion_rate has been loaded for you, calculated in the same way we saw in Chapter One. Additionally the daily_paywall_views and daily_purchases values you calculated previously have been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de33e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sensitivity = 0.1 \n",
    "\n",
    "# Find the conversion rate when increased by the percentage of the sensitivity above\n",
    "small_conversion_rate = conversion_rate * (1 + small_sensitivity) \n",
    "\n",
    "# Apply the new conversion rate to find how many more users per day that translates to\n",
    "small_purchasers = daily_paywall_views * small_conversion_rate\n",
    "\n",
    "# Subtract the initial daily_purcahsers number from this new value to see the lift\n",
    "purchaser_lift = small_purchasers - daily_purchases\n",
    "\n",
    "print(small_conversion_rate)\n",
    "print(small_purchasers)\n",
    "print(purchaser_lift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ab4468",
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_sensitivity = 0.2\n",
    "\n",
    "# Find the conversion rate when increased by the percentage of the sensitivity above\n",
    "medium_conversion_rate = conversion_rate * (1 + medium_sensitivity) \n",
    "\n",
    "# Apply the new conversion rate to find how many more users per day that translates to\n",
    "medium_purchasers = daily_paywall_views * medium_conversion_rate\n",
    "\n",
    "# Subtract the initial daily_purcahsers number from this new value to see the lift\n",
    "purchaser_lift = medium_purchasers - daily_purchases\n",
    "\n",
    "print(medium_conversion_rate)\n",
    "print(medium_purchasers)\n",
    "print(purchaser_lift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e7fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_sensitivity = 0.5\n",
    "\n",
    "# Find the conversion rate lift with the sensitivity above\n",
    "large_conversion_rate = conversion_rate * (1 + large_sensitivity)\n",
    "\n",
    "# Find how many more users per day that translates to\n",
    "large_purchasers = daily_paywall_views * large_conversion_rate\n",
    "purchaser_lift = large_purchasers - daily_purchases\n",
    "\n",
    "print(large_conversion_rate)\n",
    "print(large_purchasers)\n",
    "print(purchaser_lift)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e333a",
   "metadata": {},
   "source": [
    "#### Standard error\n",
    "Previously we observed how to calculate the standard deviation using the .std() method. In this exercise, you will explore how to calculate standard deviation for a conversion rate, which requires a slightly different procedure. You will calculate this step by step in this exercise.\n",
    "\n",
    "Loaded for you is our inner merged dataset purchase_data as well as the computed conversion_rate value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b93b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of paywall views \n",
    "n = purchase_data.purchase.count()\n",
    "\n",
    "# Calculate the quantitiy \"v\"\n",
    "v = conversion_rate * (1 - conversion_rate) \n",
    "\n",
    "# Calculate the variance and standard error of the estimate\n",
    "var = v / n\n",
    "se = var**0.5\n",
    "\n",
    "print(var)\n",
    "print(se)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee6708f",
   "metadata": {},
   "source": [
    "#### Exploring the power calculation\n",
    "As discussed, power is the probability of rejecting the null hypothesis when the alternative hypothesis is true. Here you will explore some properties of the power function and see how it relates to sample size among other parameters. The get_power() function has been included and takes the following arguments in the listed order n for sample size, p1 as the baseline value, p2 as the value with lift included, and cl as the confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0147d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The get_power() function has been included and takes the following arguments in the listed order n for sample size, \n",
    "p1 as the baseline value, p2 as the value with lift included, and cl as the confidence level.\n",
    "'''\n",
    "\n",
    "# Look at the impact of sample size increase on power\n",
    "n_param_one = get_power(n=1000, p1=p1, p2=p2, cl=cl)\n",
    "n_param_two = get_power(n=2000, p1=p1, p2=p2, cl=cl)\n",
    "\n",
    "# Look at the impact of confidence level increase on power\n",
    "alpha_param_one = get_power(n=n1, p1=p1, p2=p2, cl=0.8)\n",
    "alpha_param_two = get_power(n=n1, p1=p1, p2=p2, cl=0.95)\n",
    "    \n",
    "# Compare the ratios\n",
    "print(n_param_two / n_param_one)\n",
    "print(alpha_param_one / alpha_param_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aff325",
   "metadata": {},
   "source": [
    "##### Calculating the sample size\n",
    "You're now going to utilize the sample size function to determine how many users you need for the test and control groups under various circumstances.\n",
    "\n",
    "Included is the get_sample_size() function you viewed previously, which takes four primary arguments, power, p1, p2 and cl as described before:\n",
    "\n",
    "def get_sample_size(power, p1, p2, cl, max_n=1000000):\n",
    "    n = 1 \n",
    "    while n <= max_n:\n",
    "        tmp_power = get_power(n, p1, p2, cl)\n",
    "\n",
    "        if tmp_power >= power: \n",
    "            return n \n",
    "        else: \n",
    "            n = n + 100\n",
    "\n",
    "    return \"Increase Max N Value\"\n",
    "You will continue working with the paywall conversion rate data for this exercise, which has been pre-loaded as purchase_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd0a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the demographics and purchase data to only include paywall views\n",
    "purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])\n",
    "                            \n",
    "# Find the conversion rate\n",
    "conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())\n",
    "\n",
    "# Desired Power: 0.95\n",
    "# CL 0.90\n",
    "# Percent Lift: 0.1\n",
    "p2 = conversion_rate * (1 + 0.1)\n",
    "sample_size = get_sample_size(0.95, conversion_rate, p2, 0.90)\n",
    "print(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00749475",
   "metadata": {},
   "source": [
    "#### Confirming our test results\n",
    "To begin this chapter, you will confirm that everything ran correctly for an A/B test similar to that shown in the lesson. Like the A/B test in the lesson this one consists of trying to boost consumable sales through making changes to a paywall.\n",
    "\n",
    "The data from the test is loaded for you as \"ab_test_results\" and it has already been merged with the relevant demographics data. The checks you will perform will allow you to confidently report any results you uncover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the results\n",
    "results = ab_test_results.groupby('group').agg({'uid':pd.Series.nunique})  \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172b4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the unique users in each group \n",
    "results = ab_test_results.groupby('group').agg({'uid': pd.Series.nunique}) \n",
    "\n",
    "# Find the overall number of unique users using \"len\" and \"unique\"\n",
    "unique_users = len(ab_test_results.uid.unique()) \n",
    "\n",
    "# Find the percentage in each group\n",
    "results = results / unique_users * 100\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc0c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the unique users in each group, by device and gender\n",
    "results = ab_test_results.groupby(by=['group', 'device', 'gender']).agg({'uid': pd.Series.nunique}) \n",
    "\n",
    "# Find the overall number of unique users using \"len\" and \"unique\"\n",
    "unique_users = len(ab_test_results.uid.unique())\n",
    "\n",
    "# Find the percentage in each group\n",
    "results = results / unique_users * 100\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6738c00",
   "metadata": {},
   "source": [
    "#### Intuition behind statistical significance\n",
    "In this exercise you will work to gain an intuitive understanding of statistical significance. You will do this by utilizing the get_pvalue() function on a variety of parameter sets that could reasonably arise or be chosen during the course of an A/B test. While doing this you should observing how statistical significance results vary as you change the parameters. This will help build your intuition surrounding this concept, and reveal some of the subtle pitfalls of p-values. As a reminder, this is the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8b64f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the p-value\n",
    "p_value = get_pvalue(con_conv=0.1, test_conv=0.17, con_size=1000, test_size=1000)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bab9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the p-value\n",
    "p_value = get_pvalue(con_conv=0.1, test_conv=0.15, con_size=100, test_size=100)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5916f6b",
   "metadata": {},
   "source": [
    "#### Checking for statistical significance\n",
    "Now that you have an intuitive understanding of statistical significance and p-values, you will apply it to your test result data.\n",
    "\n",
    "The four parameters needed for the p-value function are the two conversion rates - cont_conv and test_conv and the two group sizes - cont_size and test_size. These are available in your workspace, so you have everything you need to check for statistical significance in our experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc64a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the p-value\n",
    "p_value = get_pvalue(con_conv=cont_conv, test_conv=test_conv, con_size=cont_size, test_size=test_size)\n",
    "print(p_value)\n",
    "\n",
    "# Check for statistical significance\n",
    "if p_value >= 0.05:\n",
    "    print(\"Not Significant\")\n",
    "else:\n",
    "    print(\"Significant Result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f28af8",
   "metadata": {},
   "source": [
    "#### Understanding confidence intervals\n",
    "In this exercise, you'll develop your intuition for how various parameter values impact confidence intervals. Specifically, you will explore through the get_ci() function how changes widen or tighten the confidence interval. This is the function signature, where cl is the confidence level and sd is the standard deviation.\n",
    "\n",
    "def get_ci(value, cl, sd):\n",
    "  loc = sci.norm.ppf(1 - cl/2)\n",
    "  rng_val = sci.norm.cdf(loc - value/sd)\n",
    "\n",
    "  lwr_bnd = value - rng_val\n",
    "  upr_bnd = value + rng_val \n",
    "\n",
    "  return_val = (lwr_bnd, upr_bnd)\n",
    "  return(return_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b71ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the confidence interval\n",
    "confidence_interval  = get_ci(1, 0.975, 0.5)\n",
    "print(confidence_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59262ac",
   "metadata": {},
   "source": [
    "##### Calculating confidence intervals\n",
    "Now you will calculate the confidence intervals for the A/B test results.\n",
    "\n",
    "The four values that have been calculated previously have been loaded for you (cont_conv, test_conv, test_size, cont_size) as variables with those names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b36f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of our lift distribution \n",
    "lift_mean = test_conv - cont_conv \n",
    "\n",
    "# Calculate variance and standard deviation \n",
    "lift_variance = (1 - test_conv) * test_conv /test_size + (1 - cont_conv) * cont_conv / cont_size\n",
    "lift_sd = lift_variance**0.5\n",
    "\n",
    "# Find the confidence intervals with cl = 0.95\n",
    "confidence_interval = get_ci(lift_mean, 0.95, lift_sd)\n",
    "print(confidence_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7bbdd",
   "metadata": {},
   "source": [
    "#### Plotting the distribution\n",
    "In this exercise, you will visualize the test and control conversion rates as distributions. It is helpful to practice what was covered in the example, as this may be something you have not applied before. Additionally, viewing the data in this way can give a sense of the variability inherent in our estimation.\n",
    "\n",
    "Four variables, the test and control variances (test_var, cont_var), and the test and control conversion rates (test_conv and cont_conv) have been loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc49af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the standard deviations\n",
    "control_sd = cont_var**0.5\n",
    "test_sd = test_var**0.5\n",
    "\n",
    "# Create the range of x values \n",
    "control_line = np.linspace( cont_conv - 3 * control_sd, cont_conv + 3 * control_sd , 100)\n",
    "test_line = np.linspace( test_conv - 3 * test_sd,  test_conv + 3 * test_sd , 100)\n",
    "\n",
    "# Plot the distribution \n",
    "plt.plot(control_line, sci.norm.pdf(control_line, cont_conv, control_sd))\n",
    "plt.plot(test_line, sci.norm.pdf(test_line,test_conv, test_sd))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39db361",
   "metadata": {},
   "source": [
    "#### Plotting the difference distribution\n",
    "Now lets plot the difference distribution of our results that is, the distribution of our lift.\n",
    "\n",
    "The cont_var and test_var as well as the cont_conv and test_conv have been loaded for you. Additionally the upper and lower confidence interval bounds of this distribution have been provided as lwr_ci and upr_ci respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a50c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the lift mean and standard deviation\n",
    "lift_mean = test_conv - cont_conv\n",
    "lift_sd = (test_var + cont_var) ** 0.5\n",
    "\n",
    "# Generate the range of x-values\n",
    "lift_line = np.linspace(lift_mean - 3 * lift_sd, lift_mean + 3 * lift_sd, 100)\n",
    "\n",
    "# Plot the lift distribution\n",
    "plt.plot(lift_line, sci.norm.pdf(lift_line, lift_mean, lift_sd))\n",
    "\n",
    "# Add the annotation lines\n",
    "plt.axvline(x = lift_mean, color = 'green')\n",
    "plt.axvline(x = lwr_ci, color = 'red')\n",
    "plt.axvline(x = upr_ci, color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf008d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
