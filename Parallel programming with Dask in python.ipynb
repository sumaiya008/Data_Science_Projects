{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92f053b0",
   "metadata": {},
   "source": [
    "https://campus.datacamp.com/courses/parallel-programming-with-dask-in-python/lazy-evaluation-and-parallel-computing?ex=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897bbde0",
   "metadata": {},
   "source": [
    "# chapter 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57523cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dask delayed\n",
    "from dask import delayed\n",
    "def my_square_funtion(x):\n",
    "    returnx**2\n",
    "# reate delayed version of above funtion\n",
    "delayed_square_funtion = delayed(my_square_funtion)\n",
    "\n",
    "#use the delayed funtion with input 4\n",
    "delayed_result = delayed_square_funtion(4)\n",
    "\n",
    "print(delayed_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a971745",
   "metadata": {},
   "source": [
    "##### Delaying functions\n",
    "You have been tasked with adding up your company's costs for the last 2 weeks. Because you know that you will want to run this same computation in the future with many more weeks, you think it might be a good idea to write it in a way that can be parallelized.\n",
    "\n",
    "The arrays of costs of items for the last two weeks are available as costs_week_1 and costs_week_2, and numpy has been imported as np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the delayed function from Dask\n",
    "from dask import delayed \n",
    "\n",
    "# Lazily calculate the sums of costs_week_1 and costs_week_2\n",
    "sum1 = delayed(np.sum)(costs_week_1)\n",
    "sum2 = delayed(np.sum)(costs_week_2)\n",
    "\n",
    "# Add the two delayed sums\n",
    "total = delayed(sum1 + sum2)\n",
    "\n",
    "# Compute and print the final answer\n",
    "print(total.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11058c81",
   "metadata": {},
   "source": [
    "#### Plotting the task graph\n",
    "You are trying to analyze your company's spending records. Your manager wants to see what fraction of the total spending occurred in each month. But you are going to have to run it for many files, so it would be good to set up a lazy calculation so you can speed it up using threads or processes. To figure out which of these task scheduling methods might be better for this calculation, you would like to visualize the task graph.\n",
    "\n",
    "The totals spent in two months are available for you as delayed objects as month_1_costs and month_2_costs. dask has also been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be3e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the two delayed month costs\n",
    "total_costs = month_1_costs + month_2_costs\n",
    "\n",
    "# Calculate the fraction of total cost from month 1\n",
    "month_1_fraction = month_1_costs / total_costs\n",
    "\n",
    "# Calculate the fraction of total cost from month 2\n",
    "month_2_fraction = month_2_costs / total_costs\n",
    "\n",
    "# Plot the joint task graph used to calculate the fractions\n",
    "dask.visualize(month_1_fraction, month_2_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f0502f",
   "metadata": {},
   "source": [
    "#### Analyzing songs on Spotify\n",
    "You have a list of CSV files that you want to aggregate to investigate the Spotify music catalog. Importantly, you want to be able to do this quickly and to utilize all your available computing power to do it.\n",
    "\n",
    "Each CSV file contains all the songs released in a given year, and each row gives information about an individual song.\n",
    "\n",
    "dask and delayed() have been imported for you, and the list of filenames is available in your environment as filenames. pandas has been imported as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64320123",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_songs_in_c, n_songs = 0, 0 \n",
    "\n",
    "for file in filenames:\n",
    "    # Load in the data\n",
    "    df = delayed(pd.read_csv)(file)\n",
    "    \n",
    "    # Add to running totals\n",
    "    n_songs_in_c += (df['key'] == 'C').sum()\n",
    "    n_songs += df.shape[0]\n",
    "\n",
    "# Efficiently compute total_n_songs_in_c and total_n_songs\n",
    "total_n_songs_in_c, total_n_songs = dask.compute(n_songs_in_c, n_songs)\n",
    "\n",
    "fraction_c = total_n_songs_in_c / total_n_songs\n",
    "print(total_n_songs, fraction_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ab8bc",
   "metadata": {},
   "source": [
    "##### How danceable are songs these days?\n",
    "It's time to dive deeper into the Spotify data to analyze some trends in music.\n",
    "\n",
    "In each CSV file, the 'danceability' column contains the score between 0 and 1 of how danceable each song is. The score describes how suitable a track is for dancing based on a combination of musical elements, including tempo, rhythm stability, beat strength, and overall regularity. Do you think songs are getting better or worse to dance to?\n",
    "\n",
    "dask and the delayed() function have been imported for you. pandas has been imported as pd, and matplotlib.pyplot has been imported as plt. The list of filenames is available in your environment as filenames, and the year of each file is stored in the years list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d341e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "danceabilities = []\n",
    "\n",
    "for file in filenames:\n",
    "\t# Lazily load in the data\n",
    "    df = delayed(pd.read_csv)(file)\n",
    "    # Calculate the average danceability in the file of songs\n",
    "    mean_danceability = df['danceability'].mean()\n",
    "    danceabilities.append(mean_danceability)\n",
    "\n",
    "# Compute all the mean danceabilities\n",
    "danceability_list = dask.compute(danceabilities)[0]\n",
    "# Plot the results\n",
    "plt.plot(years, danceability_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a99d84",
   "metadata": {},
   "source": [
    "#### Most popular songs\n",
    "You have one more task on this Spotify data, which is to find the top 10 most popular songs across all available years. The algorithm you will need to use to compute this is to calculate the top 10 songs in each year, and then combine these and find the top 10 of the top 10s.\n",
    "\n",
    "The following function, which finds the top 10 songs in a DataFrame, has been provided for you and is available in your environment.\n",
    "\n",
    "def top_10_most_popular(df):\n",
    "  return df.nlargest(n=10, columns='popularity')\n",
    "dask and the delayed() function have been imported for you. pandas has been imported as pd. The list of filenames is available in your environment as filenames, and the year of each file is stored in the list years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_songs = []\n",
    "\n",
    "for file in filenames:\n",
    "    df = delayed(pd.read_csv)(file)\n",
    "    # Find the top 10 most popular songs in this file\n",
    "    df_top_10 = top_10_most_popular(df)\n",
    "    top_songs.append(df_top_10)\n",
    "\n",
    "# Compute the list of top 10s\n",
    "top_songs_list = dask.compute(top_songs)[0]\n",
    "\n",
    "# Concatenate them and find the best of the best\n",
    "top_songs_df = pd.concat(top_songs_list)\n",
    "df_all_time_top_10 = top_10_most_popular(top_songs_df)\n",
    "print(df_all_time_top_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd2cb0",
   "metadata": {},
   "source": [
    "##### Loading and processing photos\n",
    "Let's say you are training a machine learning model to read American sign language images and translate them into text. The first part of this is loading images, and you have a lot of them. Thankfully, Dask can help you load and process these images lazily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf720b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the image subpackage from dask.array\n",
    "from dask.array import image\n",
    "\n",
    "# Lazily load in all jpegs inside all subdirectories inside data/asl\n",
    "image_array = image.imread('data/asl/*.jpeg')\n",
    "\n",
    "# Load only the zeroth image into memory\n",
    "zeroth_image = image_array[0].compute()\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(zeroth_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528bccc7",
   "metadata": {},
   "source": [
    "#### An image processing pipeline\n",
    "Your colleague has written a preprocessing function to use on the American sign language images in order to boost your machine learning model's accuracy. This function will take a grayscale image and run Canny edge detection on it. Canny edge detection is commonly used in classical computer vision and highlights the edges of objects in an image. You want to apply it to all of the images in your dataset.\n",
    "\n",
    "The function your colleague has written is available in your environment as compute_edges(), and it takes an image that has dimensions (1, h, w) where the height h and the width w can be any integers.\n",
    "\n",
    "The Dask array of your images is available in the environment as image_array. This array has shape (N, h, w, 3) where N is the number of images, and there are 3 channels for red, blue, and green.\n",
    "\n",
    "dask.array has been imported for you as da."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d91f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the color photos to grayscale\n",
    "grayscale_images = image_array.mean(axis=-1)\n",
    "\n",
    "# Apply the edge detection function\n",
    "edge_images = grayscale_images.map_blocks(compute_edges)\n",
    "\n",
    "# Select the zeroth image and compute its values\n",
    "sample_image = edge_images[0].compute()\n",
    "\n",
    "# Show the result\n",
    "plt.imshow(sample_image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f718bc",
   "metadata": {},
   "source": [
    "##### Creating Dask dataframes from CSVs\n",
    "Previously, you analyzed the Spotify song data using loops and delayed functions. Now you know that you can accomplish the same thing more easily using a Dask DataFrame. Let's see how much easier the same tasks you did earlier are if you do them using these methods instead of loops. First, however, you will need to load the dataset into a Dask DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f4f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dask dataframe as dd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Load in the DataFrame\n",
    "df  = dd.read_csv(\"data/spotify/*.csv\", blocksize=\"1MB\")\n",
    "\n",
    "# Convert the release_date column from string to datetime\n",
    "df['release_date'] = dd.to_datetime(df['release_date'])\n",
    "\n",
    "# Show 5 rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be18c0a2",
   "metadata": {},
   "source": [
    "#### Read Dask DataFrames from Parquet\n",
    "In Chapter 1, you analyzed some Spotify data, which was split across multiple files to find the top hits of 2005-2020. You did this using the dask.delayed() function and a loop. Let's see how much easier this analysis becomes using Dask DataFrames.\n",
    "\n",
    "dask.dataframe has been imported for you as dd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0747de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the spotify_parquet folder\n",
    "df = dd.read_parquet(\"data/spotify_parquet\")\n",
    "\n",
    "# Find the 10 most popular songs\n",
    "top_10_songs = df.nlargest(n=10, columns='popularity')\n",
    "\n",
    "# Convert the delayed result to a pandas DataFrame\n",
    "top_10_songs_df = top_10_songs.compute()\n",
    "\n",
    "print(top_10_songs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff657463",
   "metadata": {},
   "source": [
    "##### Summertime grooves\n",
    "In chapter 1, you found that the average danceability of songs has been rising since 2005. Now, you are tasked with finding the average danceability of songs released in different months. Is there some part of the year where more danceable songs are released? If you were a musician who had written a new dance song, is there a best time of year to release that? Let's find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3b5285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the months from the release_date column using its datetime accessor \n",
    "months = df['release_date'].dt.month\n",
    "\n",
    "# Group the danceabilities by month\n",
    "monthly_groupby = df['danceability'].groupby(months)\n",
    "\n",
    "# Find the mean danceability by month\n",
    "monthly_danceability = monthly_groupby.mean()\n",
    "\n",
    "# Compute the result\n",
    "monthly_danceability_result = monthly_danceability.compute()\n",
    "\n",
    "monthly_danceability_result.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89242134",
   "metadata": {},
   "source": [
    "##### Dask arrays from HDF5 datasets\n",
    "You have been tasked with analyzing European rainfall over the last 40 years. The monthly average rainfall in a grid of locations over Europe has been provided for you in HDF5 format. Since this file is pretty large, you decide to load and process it using Dask.\n",
    "\n",
    "h5py has been imported for you, and dask.array has been imported as da."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88368b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the HDF5 dataset using h5py\n",
    "hdf5_file = h5py.File('data/era_eu.hdf5')\n",
    "\n",
    "# Load the file into a Dask array with a reasonable chunk size\n",
    "precip = da.from_array(hdf5_file['/precip'], chunks=(12, 15, 15))\n",
    "\n",
    "# Select only the months of January\n",
    "january_rainfalls = precip[0::12]\n",
    "\n",
    "# Calculate the mean rainfall in January for each location\n",
    "january_mean_rainfall = january_rainfalls.mean(axis=0)\n",
    "\n",
    "plt.imshow(january_mean_rainfall.compute())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30de9b3",
   "metadata": {},
   "source": [
    "##### Dask arrays from Zarr datasets\n",
    "You are tasked with analyzing European temperatures, and are given the same dataset which was in era_eu.hdf but this time in Zarr format. Zarr is a modern, powerful dataset format for storing chunked data. It is particularly good for use on cloud computing services but is also great on your own computer.\n",
    "\n",
    "dask.array has been imported for you as da."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7121978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the temperature data from the Zarr dataset\n",
    "temps = da.from_zarr('data/era_eu.zarr', component='temp')\n",
    "\n",
    "# Print the Dask array of temperatures to see the chunk sizes\n",
    "print(temps)\n",
    "\n",
    "# Find the minimum of the mean monthly temperatures\n",
    "all_time_low = temps.min()\n",
    "\n",
    "# Compute the answer\n",
    "all_time_low_value = all_time_low.min()\n",
    "\n",
    "print(all_time_low_value, \"°C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f469e55d",
   "metadata": {},
   "source": [
    "##### Exploratory data analysis with xarray\n",
    "Xarray makes working with multi-dimensional data easier, just like pandas makes working with tabular data easier. Best of all, Xarray can use Dask in the background to help you process the data quickly and efficiently.\n",
    "\n",
    "You have been tasked with analyzing the European weather dataset further. Now that you know how to use Xarray, you will start by doing some exploratory data analysis.\n",
    "\n",
    "xarray has been imported for you as xr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0582d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the ERA5 dataset\n",
    "ds = xr.open_zarr(\"data/era_eu.zarr\")\n",
    "\n",
    "# Select the zeroth time in the DataSet\n",
    "ds_sel = ds.isel(time=0)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(8, 3))\n",
    "\n",
    "# Plot the zeroth temperature field on ax1\n",
    "ds_sel['temp'].plot(ax=ax1)\n",
    "\n",
    "# Plot the zeroth precipitation field on ax2\n",
    "ds_sel['precip'].plot(ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab2b3a6",
   "metadata": {},
   "source": [
    "##### Monthly mean temperatures\n",
    "After seeing the analysis of the average temperatures, you simply need to see the rest of the months. This would be rather annoying to do using the method we used before as it involves a lot of complicated slicing in time to select the right months. Thankfully, with Xarray, it is a lot simpler.\n",
    "\n",
    "The European weather dataset is available in your environment as the Xarray DataSet ds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd14912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the months from the time coordinates\n",
    "months = ds['time'].dt.month\n",
    "\n",
    "# Select the temp DataArray and group by months\n",
    "monthly_groupby = ds['temp'].groupby(months)\n",
    "\n",
    "# Find the mean temp by month\n",
    "monthly_mean_temps = monthly_groupby.mean()\n",
    "\n",
    "# Compute the result\n",
    "monthly_mean_temps_computed = monthly_mean_temps.compute()\n",
    "\n",
    "monthly_mean_temps_computed.plot(col='month', col_wrap=4, add_colorbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e73119",
   "metadata": {},
   "source": [
    "##### Calculating the trend in European temperatures\n",
    "You want to calculate the average European temperature from 1980 to present using the ERA5 dataset. This data is a Zarr dataset of monthly mean temperature and precipitation, on a grid of latitudes and longitudes. The Zarr file is chunked so that each subfile on the disk is an array of 15 latitudes, 15 longitudes, and 12 months.\n",
    "\n",
    "xarray has been imported for you as xr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc20f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the ERA5 dataset\n",
    "ds = xr.open_zarr('data/era_eu.zarr')\n",
    "\n",
    "# Select the temperature dataset and take the latitude and longitude mean\n",
    "temp_timeseries = ds['temp'].mean(dim=('lat', 'lon'))\n",
    "\n",
    "# Calculate the 12 month rolling mean\n",
    "temp_rolling_mean = temp_timeseries.rolling(time=12).mean()\n",
    "\n",
    "# Plot the result\n",
    "temp_rolling_mean.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8fb61c",
   "metadata": {},
   "source": [
    "##### Creating a Dask bag\n",
    "You have been tasked with analyzing some reviews left on TripAdvisor. Your colleague has provided the reviews as a list of strings. You want to use Dask to speed up your analysis of the data, so to start with, you need to load the data into a Dask bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfcb177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Dask bag subpackage as db\n",
    "import dask.bag as db\n",
    "\n",
    "# Convert the list to a Dask bag\n",
    "review_bag = db.from_sequence(reviews_list, npartitions=3)\n",
    "\n",
    "# Print 1 element of the bag\n",
    "print(review_bag.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe316a84",
   "metadata": {},
   "source": [
    "##### Creating a bag from saved text\n",
    "This time your colleague has saved the reviews to some text files. There are multiple files and multiple reviews in each file. Each review is on a separate line of the text file.\n",
    "\n",
    "You want to load these into Dask lazily so you can use parallel processing to analyze them more quickly.\n",
    "\n",
    "dask.bag has been imported for you as db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a7cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all the .txt files inside data/tripadvisor_hotel_reviews\n",
    "review_bag = db.read_text(\"data/tripadvisor_hotel_reviews/*.txt\")\n",
    "\n",
    "# Count the number of reviews in the bag\n",
    "review_count = review_bag.count()\n",
    "\n",
    "# Compute and print the answer\n",
    "print(review_count.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bc46fe",
   "metadata": {},
   "source": [
    "#### String operations\n",
    "Now that you can load the text data into bags, it is time to actually do something with it. To detect how positive or negative the reviews are, you will start by counting some keywords.\n",
    "\n",
    "The bag you created in the last exercise, review_bag, is available in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d2155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of the reviews to lower case\n",
    "lowercase_reviews = review_bag.str.lower()\n",
    "\n",
    "# Count the number of times 'excellent' appears in each review\n",
    "excellent_counts = lowercase_reviews.str.count('excellent')\n",
    "\n",
    "# Print the first 10 counts of 'excellent'\n",
    "print(excellent_counts.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd923063",
   "metadata": {},
   "source": [
    "#### Loading JSON data\n",
    "You have been asked to analyze some data about politicians from different countries. This data is stored in JSON format. The first step you need to accomplish is to load it in and convert it from string data to dictionaries.\n",
    "\n",
    "dask.bag has been imported for you as db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6456c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of the json package\n",
    "import json\n",
    "\n",
    "# Read all of the JSON files inside data/politicians\n",
    "text_bag = db.read_text(\"data/politicians/*.json\")\n",
    "\n",
    "# Convert the JSON strings into dictionaries\n",
    "dict_bag = text_bag.map(json.loads)\n",
    "\n",
    "# Show an example dictionary\n",
    "print(dict_bag.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c061c8",
   "metadata": {},
   "source": [
    "#### Filtering Dask bags\n",
    "The politician data you are working with comes from different sources, so it isn't very clean. Many of the dictionaries are missing keys that you may need to run your analysis. You will need to filter out the elements with important missing keys.\n",
    "\n",
    "A function named has_birth_date() is available in the environment. It checks the input dictionary to see if it contains the key 'birth_date'. It returns True if the key is in the dictionary and False if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4386fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of elements in dict_bag\n",
    "print(dict_bag.count().compute())\n",
    "\n",
    "# Filter out records using the has_birth_date() function\n",
    "filtered_bag = dict_bag.filter(has_birth_date)\n",
    "\n",
    "# Print the number of elements in filtered_bag\n",
    "print(filtered_bag.count().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd833f9",
   "metadata": {},
   "source": [
    "##### Chaining operations\n",
    "Now that you have loaded and cleaned the data, you can begin analyzing it. Your first task is to look at the birth dates of the politicians. The birth dates are in string format like 'YYYY-MM-DD'. The first 4 characters in the string are the year.\n",
    "\n",
    "The filtered Dask bag you created in the last exercise, filtered_bag, is available in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa527f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 'birth_date' from each dictionary in the bag\n",
    "birth_date_bag = filtered_bag.pluck('birth_date')\n",
    "\n",
    "# Extract the year as an integer from the birth_date strings\n",
    "birth_year_bag = birth_date_bag.map(lambda x: int(x[:4]))\n",
    "\n",
    "# Calculate the min, max and mean birth years\n",
    "min_year = birth_year_bag.min()\n",
    "max_year = birth_year_bag.max()\n",
    "mean_year = birth_year_bag.mean()\n",
    "\n",
    "# Compute the results efficiently and print them\n",
    "print(dask.compute(min_year, max_year, mean_year))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8485313c",
   "metadata": {},
   "source": [
    "##### Restructuring a dictionary\n",
    "Now you want to clean up the politician data and move it into a Dask DataFrame. However, the politician data is nested, so you will need to process it some more before it fits into a DataFrame.\n",
    "\n",
    "One particular piece of data you want to extract is buried a few layers inside the dictionary. This is a link to a website for each politician. The example below shows how it is stored inside the dictionary.\n",
    "\n",
    "record = {\n",
    "...\n",
    " 'links': [{'note': '...',\n",
    "            'url': '...'},],  # Stored here\n",
    "...\n",
    "}\n",
    "The bag of politician data is available in your environment as dict_bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_url(x):\n",
    "    # Extract the url and assign it to the key 'url'\n",
    "    x['url'] = x['links'][0]['url']\n",
    "    return x\n",
    "  \n",
    "# Run the function on all elements in the bag.\n",
    "dict_bag = dict_bag.map(extract_url)\n",
    "\n",
    "print(dict_bag.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110c18fc",
   "metadata": {},
   "source": [
    "#### Converting to DataFrame\n",
    "You want to make a DataFrame out of the politician JSON data. Now that you have de-nested the data, all you need to do is select the keys to keep as columns in the DataFrame.\n",
    "\n",
    "The Dask bag you created in the last exercise is available in your environment as dict_bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cd1b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_keys(dictionary, keys_to_keep):\n",
    "  new_dict = {}\n",
    "  # Loop through kept keys and add them to new dictionary\n",
    "  for k in keys_to_keep:\n",
    "    new_dict[k] = dictionary[k]\n",
    "  return new_dict\n",
    "\n",
    "# Use the select_keys to reduce to the 4 required keys\n",
    "filtered_bag = dict_bag.map(select_keys, keys_to_keep=['gender','name', 'birth_date', 'url'])\n",
    "\n",
    "# Convert the restructured bag to a DataFrame\n",
    "df = filtered_bag.to_dataframe()\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02778dc3",
   "metadata": {},
   "source": [
    "##### Loading wav data\n",
    "To work with any non-standard data using Dask bags, you will need to write a lot of functions yourself. For this task, you are analyzing audio data, and so you need a custom function to load it.\n",
    "\n",
    "Some of the audio recordings failed, and the audio is silent in these. Regular audio data looks like a wave, where the amplitude goes to large positive and negative values. Therefore, to check if a recording is silent, you can check whether the audio clip has very small amplitudes overall.\n",
    "\n",
    "The scipy.io.wavfile module has been imported into your environment as wavfile, and numpy has been imported as np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe1f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav(filename):\n",
    "    # Load in the audio data\n",
    "    sampling_freq, audio = wavfile.read(filename)\n",
    "    \n",
    "    # Add the filename, audio data, and sampling frequency to the dictionary\n",
    "    data_dict = {\n",
    "        'filename': filename,\n",
    "        'audio': audio, \n",
    "        'sample_frequency': sampling_freq\n",
    "    }\n",
    "    return data_dict\n",
    "\n",
    "def not_silent(data_dict):\n",
    "    # Check if the audio data is silent\n",
    "    return np.mean(np.abs(data_dict['audio'])) > 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b3454",
   "metadata": {},
   "source": [
    "#### Constructing custom Dask bags\n",
    "A common use case for Dask bags is to convert some code you have already written to run in parallel. Depending on the code, sometimes it can be easier to construct lists of delayed objects and then convert them to a bag. Other times it will be easier to form a Dask bag early on in the code and map functions over it. Which of these options is easier will depend on your exact code, so it's important that you know how to use either method.\n",
    "\n",
    "dask has been imported for you, and dask.bag has been imported as db. A list of file names strings is available in your environment as wavfiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e2772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of filenames into a Dask bag\n",
    "filename_bag = db.from_sequence(wavfiles)\n",
    "\n",
    "# Apply the load_wav() function to each element of the bag\n",
    "loaded_audio_bag = filename_bag.map(load_wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_loaded_audio = []\n",
    "\n",
    "for wavfile in wavfiles:\n",
    "    # Append the delayed loaded audio to the list\n",
    "    delayed_loaded_audio.append(dask.delayed(load_wav)(wavfile))\n",
    "\n",
    "# Convert the list to a Dask bag\n",
    "loaded_audio_bag = db.from_delayed(delayed_loaded_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef2ab2",
   "metadata": {},
   "source": [
    "#### Processing unstructured audio\n",
    "You have a lot of .wav files to process, which could take a long time. Fortunately, the functions you just wrote can be used with Dask bags to run the analysis in parallel using all your available cores.\n",
    "\n",
    "Here are descriptions of the not_silent() function you wrote, plus two extras you can use.\n",
    "\n",
    "not_silent(audio_dict) - Takes an audio dictionary, and checks if the audio isn't silent. Returns True/False.\n",
    "peak_frequency(audio_dict) - Takes a dictionary of audio data, analyzes it to find the peak frequency of the audio, and adds it to the dictionary.\n",
    "delete_dictionary_entry(dict, key_to_drop) - Deletes a given key from the input dictionary.\n",
    "The audio data loaded_audio_bag is available in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59787b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out blank audio files\n",
    "filtered_audio_bag = loaded_audio_bag.filter(not_silent)\n",
    "\n",
    "# Apply the peak_frequency function to all audio files\n",
    "audio_and_freq_bag = filtered_audio_bag.map(peak_frequency)\n",
    "\n",
    "# Use the delete_dictionary_entry function to drop the audio\n",
    "final_bag = audio_and_freq_bag.map(delete_dictionary_entry, key_to_drop='audio')\n",
    "\n",
    "# Convert to a DataFrame and run the computation\n",
    "df = final_bag.to_dataframe().compute()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9504d3ca",
   "metadata": {},
   "source": [
    "#### Clusters and clients\n",
    "Depending on your computer hardware and the calculation you are trying to complete, it may be faster to run it using a mixture of threads and processes. To do this, you need to set up a local cluster.\n",
    "\n",
    "There are two ways to set up a local cluster that Dask will use. The first way is to create the local cluster and pass it to a client. This is very similar to how you would set up a client to run across a cluster of computers! The second way is to use the client directly and allow it to create the local cluster itself. This is a shortcut that works for local clusters, but not for the other types of cluster.\n",
    "\n",
    "In this exercise, you will create clients using both methods.\n",
    "\n",
    "Be careful when creating the cluster and clients. If you configure them incorrectly, your session may time out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a09716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Client and LocalCluster\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Create a thread-based local cluster\n",
    "cluster = LocalCluster(\n",
    "\tprocesses=False, \n",
    "    n_workers=4,\n",
    "    threads_per_worker=1\n",
    ")\n",
    "\n",
    "# Create a client\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# Create a client without creating cluster first\n",
    "client = Client(\n",
    "\tprocesses=False, \n",
    "    n_workers=4,\n",
    "    threads_per_worker=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44428aef",
   "metadata": {},
   "source": [
    "#### Using Dask to train a linear model\n",
    "Dask can be used to train machine learning models on datasets that are too big to fit in memory, and allows you to distribute the data loading, preprocessing, and training across multiple threads, processes, and even across multiple computers.\n",
    "\n",
    "You have been tasked with training a machine learning model which will predict the popularity of songs in the Spotify dataset you used in previous chapters. The data has already been loaded as lazy Dask DataFrames. The input variables are available as dask_X and contain a few numeric columns, such as the song's tempo and danceability. The target values are available as dask_y and are the popularity score of each song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SGDRegressor and the Incremental wrapper\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from dask_ml.wrappers import Incremental\n",
    "\n",
    "# Create a SGDRegressor model\n",
    "model = SGDRegressor()\n",
    "\n",
    "# Wrap the model so that it works with Dask\n",
    "dask_model = Incremental(model, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the wrapped model\n",
    "dask_model.fit(dask_X, dask_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7adb1b2",
   "metadata": {},
   "source": [
    "#### Making lazy predictions\n",
    "The model you trained last time was good, but it could be better if you passed through the training data a few more times. Also, it is a shame to see a good model go to waste, so you should use this one to make some predictions on a separate dataset from the one you train on.\n",
    "\n",
    "An unfitted version of the model you created in the last exercise is available in your environment as dask_model. Dask DataFrames of training data are available as dask_X and dask_y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf572ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the training data 5 times\n",
    "for i in range(5):\n",
    "\tdask_model.partial_fit(dask_X, dask_y)\n",
    "\n",
    "# Use your model to make predictions\n",
    "y_pred_delayed = dask_model.predict(dask_X)\n",
    "\n",
    "# Compute the predictions\n",
    "y_pred_computed = y_pred_delayed.compute()\n",
    "\n",
    "print(y_pred_computed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d4cf0b",
   "metadata": {},
   "source": [
    "#### Lazily transforming training data\n",
    "Preprocessing your input variables is a vital step in machine learning and will often improve the accuracy of the model you create. In the last couple of exercises, the Spotify data was preprocessed for you, but it is important that you know how to do it yourself.\n",
    "\n",
    "In this exercise, you will use the StandardScaler() scaler object, which transforms columns of an array so that they have a mean of zero and standard deviation of one.\n",
    "\n",
    "The Dask DataFrame of Spotify songs is available in your environment as dask_df. It contains both the target popularity scores and the input variables which you used to predict these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1f2b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the StandardScaler class\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "\n",
    "X = dask_df[['duration_ms', 'explicit', 'danceability', 'acousticness', 'instrumentalness', 'tempo']]\n",
    "\n",
    "# Select the target variable\n",
    "y = dask_df['popularity']\n",
    "\n",
    "# Create a StandardScaler object and fit it on X\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "# Transform X\n",
    "X = scaler.transform(X)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1ea356",
   "metadata": {},
   "source": [
    "#### Lazy train-test split\n",
    "You have transformed the X variables. Now you need to finish your data prep by transforming the y variables and splitting your data into train and test sets.\n",
    "\n",
    "The variables X and y, which you created in the last exercise, are available in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a37a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train_test_split function\n",
    "from dask_ml.model_selection import train_test_split\n",
    "\n",
    "# Rescale the target values\n",
    "y = y / 100\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74086697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
