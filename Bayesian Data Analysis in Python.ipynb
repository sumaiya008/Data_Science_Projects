{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6955811",
   "metadata": {},
   "source": [
    "https://app.datacamp.com/learn/courses/bayesian-data-analysis-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b2ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0731371c",
   "metadata": {},
   "source": [
    "#### Probability distributions\n",
    "\n",
    "This exercise will test your ability to visualize and interpret probability distributions. You have been given a long list of draws from a distribution of the heights of plants in centimeters, contained in the variable draws. seaborn and matplotlib.pyplot have been imported for you as sns and plt, respectively. Time to get your hands dirty with data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7765ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Print the list of draws\n",
    "print(draws)\n",
    "\n",
    "# Print the length of draws\n",
    "print(len(draws))\n",
    "\n",
    "# Plot the density of draws\n",
    "sns.kdeplot(draws, shade=True)\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1be38b",
   "metadata": {},
   "source": [
    "OR = additon\n",
    "AND = multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b28896",
   "metadata": {},
   "source": [
    "__Let's play cards__\n",
    "\n",
    "You have a regular deck of 52 well-shuffled playing cards. The deck consists of 4 suits, and there are 13 cards in each suite: ranks 2 through 10, a jack, a queen, a king, and an ace. This means that in the whole deck of 52, there are four of each distinct rank: four aces, four kings, four tens, four fives, etc.\n",
    "\n",
    "Since there are 52 distinct cards, the probability of drawing any one particular card is 1/52. Using the two rules of probability you've learned about in the last video, calculate the probabilities of drawing some specific combinations of cards, as described in the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70938525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probability of drawing <= 5\n",
    "p_five_or_less = 4 / 52 + 4 / 52 + 4 / 52 + 4 / 52\n",
    "print(p_five_or_less)\n",
    "# Calculate probability of drawing four aces\n",
    "p_all_four_aces = 4 / 52 * 3 / 51 * 2 / 50 * 1 / 49\n",
    "print(p_all_four_aces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6b77d8",
   "metadata": {},
   "source": [
    "__Bayesian spam filter__\n",
    "\n",
    "Well done on the previous exercise! Let's now tackle the famous Bayes' Theorem and use it for a simple but important task: spam detection.\n",
    "\n",
    "While browsing your inbox, you have figured out that quite a few of the emails you would rather not waste your time on reading contain exclamatory statements, such as \"BUY NOW!!!\". You start thinking that the presence of three exclamation marks next to each other might be a good spam predictor! Hence you've prepared a DataFrame called emails with two variables: spam, whether the email was spam, and contains_3_exlc, whether it contains the string \"!!!\". The head of the data looks like this:\n",
    "\n",
    "     spam    contains_3_excl\n",
    "0    False             False\n",
    "1    False             False\n",
    "2    True              False\n",
    "3    False             False\n",
    "4    False             False\n",
    "Your job is to calculate the probability of the email being spam given that it contains three exclamation marks. Let's tackle it step by step! Here is Bayes' formula for your reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bab1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the unconditional probability of spam\n",
    "p_spam = emails[\"spam\"].mean()\n",
    "print(p_spam)\n",
    "\n",
    "# Calculate and print the unconditional probability of \"!!!\"\n",
    "p_3_excl = emails[\"contains_3_excl\"].mean()\n",
    "print(p_3_excl)\n",
    "\n",
    "# Calculate and print the probability of \"!!!\" given spam\n",
    "p_3_excl_given_spam = emails.loc[emails[\"spam\"]][\"contains_3_excl\"].mean()\n",
    "print(p_3_excl_given_spam)\n",
    "\n",
    "# Calculate and print the probability of spam given \"!!!\"\n",
    "p_spam_given_3_excl = p_3_excl_given_spam * p_spam / p_3_excl\n",
    "\n",
    "print(p_spam_given_3_excl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796ad563",
   "metadata": {},
   "source": [
    "__Tossing a coin__\n",
    "\n",
    "In the video, you have seen our custom get_heads_prob() function that estimates the probability of success of a binomial distribution. In this exercise, you will use it yourself and verify whether it does its job well in a coin-flipping experiment.\n",
    "\n",
    "Watch out for the confusion: there are two different probability distributions involved! One is the binomial, which we use to model the coin-flipping. It's a discrete distribution with two possible values (heads or tails) parametrized with the probability of success (tossing heads). The Bayesian estimate of this parameter is another, continuous probability distribution. We don't know what kind of distribution it is, but we can estimate it with get_heads_prob() and visualize it.\n",
    "\n",
    "numpy and seaborn have been imported for you as np and sns, respectively.\n",
    "\n",
    "Generate a list of 1000 coin tosses (0s and 1s) with 50% chance of tossing heads, and assign to the variable tosses.\n",
    "Use the tosses and the get_heads_prob() function to estimate the heads probability, and assign the result to heads_prob.\n",
    "Draw a density plot of the distribution of the heads probability you have just estimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e387414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 1000 coin tosses\n",
    "tosses = np.random.binomial(1, 0.5, size=1000)\n",
    "# Estimate the heads probability\n",
    "heads_prob = get_heads_prob(tosses)\n",
    "\n",
    "# Plot the distribution of heads probability\n",
    "sns.kdeplot(heads_prob, shade=True, label=\"heads probabilty\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e693c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate and plot heads probability based on no data\n",
    "heads_prob_nodata = get_heads_prob([])\n",
    "sns.kdeplot(heads_prob_nodata, shade=True, label=\"no data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe08d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate and plot heads probability based on a single tails\n",
    "heads_prob_onetails = get_heads_prob([0])\n",
    "sns.kdeplot(heads_prob_onetails, shade=True, label=\"single tails\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44317832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate and plot heads probability based on 1000 tosses with a biased coin\n",
    "biased_tosses = np.random.binomial(1, 0.05, size=1000)\n",
    "heads_prob_biased = get_heads_prob(biased_tosses)\n",
    "sns.kdeplot(heads_prob_biased, shade=True, label=\"biased coin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d3dc0",
   "metadata": {},
   "source": [
    "#### Hey, is this coin fair?\n",
    "In the last two exercises, you have examined the get_heads_prob() function to discover how the model estimates the probability of tossing heads and how it updates its estimate as more data comes in.\n",
    "\n",
    "Now, let's get down to some serious stuff: would you like to play coin flipping against your friend? She is willing to play, as long as you use her special lucky coin. The tosses variable contains a list of 1000 results of tossing her coin. Will you play?\n",
    "\n",
    "In this exercise, you will be doing some plotting with the seaborn package again, which has been imported for you as sns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a117a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign first 10 and 100 tosses to separate variables\n",
    "tosses_first_10 = tosses[:10]\n",
    "tosses_first_100 = tosses[:100]\n",
    "\n",
    "# Get head probabilities for first 10, first 100, and all tossses\n",
    "heads_prob_first_10 = get_heads_prob(tosses_first_10)\n",
    "heads_prob_first_100 = get_heads_prob(tosses_first_100)\n",
    "heads_prob_all = get_heads_prob(tosses)\n",
    "\n",
    "# Plot density of head probability for each subset of tosses\n",
    "sns.kdeplot(heads_prob_first_10, shade=True, label=\"first_10\")\n",
    "sns.kdeplot(heads_prob_first_100, shade=True, label=\"first_100\")\n",
    "sns.kdeplot(heads_prob_all, shade=True, label=\"all\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fe9f4a",
   "metadata": {},
   "source": [
    "#### Towards grid approximation\n",
    "Congratulations! You have just been hired as a data analyst at your government's Department of Health. The cabinet is considering the purchase of a brand-new drug against a deadly and contagious virus. There are some doubts, however, regarding how effective the new drug is against the virus. You have been tasked with estimating the drug's efficacy rate, i.e. the percentage of patients cured by the drug.\n",
    "\n",
    "An experiment was quickly set up in which 10 sick patients have been treated with the drug. Once you know how many of them are cured, you can use the binomial distribution with a cured patient being a \"success\" and the efficacy rate being the \"probability of success\". While you are waiting for the experiment's results, you decide to prepare the parameter grid.\n",
    "\n",
    "numpy and pandas have been imported for you as np and pd, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5e71e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cured patients array from 1 to 10\n",
    "num_patients_cured = np.arange(0, 11, 1)\n",
    "# Create efficacy rate array from 0 to 1 by 0.01\n",
    "efficacy_rate = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "# Combine the two arrays in one DataFrame\n",
    "df = pd.DataFrame([(x, y) for x in num_patients_cured for y in efficacy_rate])\n",
    "\n",
    "# Name the columns\n",
    "df.columns = [\"num_patients_cured\", \"efficacy_rate\"]\n",
    "\n",
    "# Print df\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dbcf09",
   "metadata": {},
   "source": [
    "#### Grid approximation without prior knowledge\n",
    "According to the experiment's outcomes, out of 10 sick patients treated with the drug, 9 have been cured. What can you say about the drug's efficacy rate based on such a small sample? Assume you have no prior knowledge whatsoever regarding how good the drug is.\n",
    "\n",
    "A DataFrame df with all possible combinations of the number of patients cured and the efficacy rate which you created in the previous exercise is available in the workspace.\n",
    "\n",
    "uniform and binom have been imported for you from scipy.stats. Also, pandas and seaborn are imported as pd and sns, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e8a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the prior efficacy rate and the likelihood\n",
    "df[\"prior\"] = uniform.pdf(df[\"efficacy_rate\"])\n",
    "df[\"likelihood\"] = binom.pmf(df[\"num_patients_cured\"], 10, df[\"efficacy_rate\"])\n",
    "\n",
    "# Calculate the posterior efficacy rate and scale it to sum up to one\n",
    "df[\"posterior_prob\"] = df[\"prior\"] * df[\"likelihood\"]\n",
    "df[\"posterior_prob\"] /= df[\"posterior_prob\"].sum()\n",
    "\n",
    "# Compute the posterior probability of observing 9 cured patients\n",
    "df_9_of_10_cured = df.loc[df[\"num_patients_cured\"] == 9]\n",
    "df_9_of_10_cured[\"posterior_prob\"] /= df_9_of_10_cured[\"posterior_prob\"].sum()\n",
    "\n",
    "# Plot the drug's posterior efficacy rate\n",
    "sns.lineplot(df_9_of_10_cured[\"efficacy_rate\"], df_9_of_10_cured[\"posterior_prob\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f501a",
   "metadata": {},
   "source": [
    "#### Updating posterior belief\n",
    "Well done on estimating the posterior distribution of the efficacy rate in the previous exercise! Unfortunately, due to a small data sample, this distribution is quite wide, indicating much uncertainty regarding the drug's quality. Luckily, testing of the drug continues, and a group of another 12 sick patients have been treated, 10 of whom were cured. We need to update our posterior distribution with these new data!\n",
    "\n",
    "This is easy to do with the Bayesian approach. We simply need to run the grid approximation similarly as before, but with a different prior. We can use all our knowledge about the efficacy rate (embodied by the posterior distribution from the previous exercise) as a new prior! Then, we recompute the likelihood for the new data, and get the new posterior!\n",
    "\n",
    "The DataFrame you created in the previous exercise, df, is available in the workspace and binom has been imported for you from scipy.stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf855a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign old posterior to new prior and calculate likelihood\n",
    "df[\"new_prior\"] = df[\"posterior_prob\"]\n",
    "df[\"new_likelihood\"] = binom.pmf(df[\"num_patients_cured\"], 12, df[\"efficacy_rate\"])\n",
    "\n",
    "# Calculate new posterior and scale it\n",
    "df[\"new_posterior_prob\"] = df[\"new_prior\"] * df[\"new_likelihood\"]\n",
    "df[\"new_posterior_prob\"] /= df[\"new_posterior_prob\"].sum()\n",
    "\n",
    "# Compute the posterior probability of observing 10 cured patients\n",
    "df_10_of_12_cured = df.loc[df[\"num_patients_cured\"] == 10]\n",
    "df_10_of_12_cured[\"new_posterior_prob\"] /= df_10_of_12_cured[\"new_posterior_prob\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39193c71",
   "metadata": {},
   "source": [
    "#### Simulating posterior draws\n",
    "You have just decided to use a Beta(5, 2) prior for the efficacy rate. You are also using the binomial distribution to model the data (curing a sick patient is a \"success\", remember?). Since the beta distribution is a conjugate prior for the binomial likelihood, you can simply simulate the posterior!\n",
    "\n",
    "You know that if the prior is , then the posterior is , with:\n",
    "\n",
    ",\n",
    "\n",
    ".\n",
    "\n",
    "Can you simulate the posterior distribution? Recall that altogether you have data on 22 patients, 19 of whom have been cured. numpy and seaborn have been imported for you as np and sns, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a200e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of patients treated and cured\n",
    "num_patients_treated = 10 + 12\n",
    "num_patients_cured = 9 + 10\n",
    "\n",
    "# Simulate 10000 draws from the posterior distribuition\n",
    "posterior_draws = np.random.beta(num_patients_cured + 5, num_patients_treated - num_patients_cured + 2, 10000)\n",
    "\n",
    "# Plot the posterior distribution\n",
    "sns.kdeplot(posterior_draws, shade=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc26bcc",
   "metadata": {},
   "source": [
    "#### Point estimates\n",
    "You continue working at your government's Department of Health. You have been tasked with filling the following memo with numbers, before it is sent to the secretary.\n",
    "\n",
    "Based on the experiments carried out by ourselves and neighboring countries, should we distribute the drug, we can expect ___ infected people to be cured. There is a 50% probability the number of cured infections will amount to at least ___, and with 90% probability it will not be less than ___.\n",
    "\n",
    "The array of posterior draws of the drug's efficacy rate you have estimated before is available to you as drug_efficacy_posterior_draws.\n",
    "\n",
    "Calculate the three numbers needed to fill in the memo, knowing there are 100,000 infections at the moment. numpy has been imported for you as np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb28afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the expected number of people cured\n",
    "cured_expected = np.mean(drug_efficacy_posterior_draws) * 100_000\n",
    "\n",
    "# Calculate the minimum number of people cured with 50% probability\n",
    "min_cured_50_perc = np.median(drug_efficacy_posterior_draws) * 100_000\n",
    "\n",
    "# Calculate the minimum number of people cured with 90% probability\n",
    "min_cured_90_perc = np.percentile(drug_efficacy_posterior_draws, 10) * 100_000\n",
    "\n",
    "# Print the filled-in memo\n",
    "print(f\"Based on the experiments carried out by ourselves and neighboring countries, \\nshould we distribute the drug, we can expect {int(cured_expected)} infected people to be cured. \\nThere is a 50% probability the number of cured infections \\nwill amount to at least {int(min_cured_50_perc)}, and with 90% probability \\nit will not be less than {int(min_cured_90_perc)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851e59f",
   "metadata": {},
   "source": [
    "#### Highest Posterior Density credible intervals\n",
    "You know that reporting bare point estimates is not enough. It would be great to provide a measure of uncertainty in the drug's efficacy rate estimate, and you have all the means to do so. You decide to add the following to the memo.\n",
    "\n",
    "The experimental results indicate that with a 90% probability the new drug's efficacy rate is between ___ and ___, and with a 95% probability it is between ___ and ___.\n",
    "\n",
    "You will need to calculate two credible intervals: one of 90% and another of 95% probability. The drug_efficacy_posterior_draws array is still available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a7ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import arviz as az\n",
    "import arviz as az\n",
    "\n",
    "# Calculate HPD credible interval of 90%\n",
    "ci_90 = az.hdi(drug_efficacy_posterior_draws, hdi_prob=0.9)\n",
    "\n",
    "# Calculate HPD credible interval of 95%\n",
    "ci_95 = az.hdi(drug_efficacy_posterior_draws, hdi_prob=0.95)\n",
    "\n",
    "# Print the memo\n",
    "print(f\"The experimental results indicate that with a 90% probability \\nthe new drug's efficacy rate is between {np.round(ci_90[0], 2)} and {np.round(ci_90[1], 2)}, \\nand with a 95% probability it is between {np.round(ci_95[0], 2)} and {np.round(ci_95[1], 2)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa77742",
   "metadata": {},
   "source": [
    "#### Simulate beta posterior\n",
    "In the upcoming few exercises, you will be using the simulate_beta_posterior() function you saw defined in the last video. In this exercise, you will get a feel for what the function is doing by carrying out the computations it performs.\n",
    "\n",
    "You are given a list of ten coin tosses, called tosses, in which 1 stands for heads, 0 for tails, and we define heads as a \"success\". To simulate the posterior probability of tossing heads, you will use a beta prior. Recall that if the prior is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3131e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set prior parameters and calculate number of successes\n",
    "beta_prior_a = 1\n",
    "beta_prior_b = 10\n",
    "num_successes = np.sum(tosses)\n",
    "\n",
    "# Generate posterior draws\n",
    "posterior_draws = np.random.beta(\n",
    "  num_successes + beta_prior_a, \n",
    "  len(tosses) - num_successes + beta_prior_b, \n",
    "  10000)  \n",
    "\n",
    "# Plot density of posterior_draws\n",
    "sns.kdeplot(posterior_draws, shade=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7ede67",
   "metadata": {},
   "source": [
    "#### Posterior click rates\n",
    "After a successful career episode at the Department for Health, you switch to marketing. Your new company has just run two pilot advertising campaigns: one for sneakers, and one for clothes. Your job is to find out which one was more effective as measured by the click-through rate and should be rolled out to a larger audience.\n",
    "\n",
    "You decide to run A/B testing, modeling the data using the binomial likelihood. You found out that a typical click-through rate for the previous ads has been around 15% recently, with results varying between 5% and 30%. Based on this, you conclude that  would be a good prior for the click-through rate.\n",
    "\n",
    "The ads data, the simulate_beta_posterior() function you saw in the video, and numpy (as np) are available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe59dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prior draws\n",
    "prior_draws = np.random.beta(10, 50, 100000)\n",
    "\n",
    "# Plot the prior\n",
    "sns.kdeplot(prior_draws, shade=True, label=\"prior\")\n",
    "plt.show()\n",
    "\n",
    "# Extract the banner_clicked column for each product\n",
    "clothes_clicked = ads.loc[ads[\"product\"] == \"clothes\"][\"banner_clicked\"]\n",
    "sneakers_clicked = ads.loc[ads[\"product\"] == \"sneakers\"][\"banner_clicked\"]\n",
    "\n",
    "# Simulate posterior draws for each product\n",
    "clothes_posterior = simulate_beta_posterior(clothes_clicked, 10, 50)\n",
    "sneakers_posterior = simulate_beta_posterior(sneakers_clicked, 10, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda3736",
   "metadata": {},
   "source": [
    "#### A or B, and how sure are we?\n",
    "You have just discovered that clothes ads are likely to have a higher click ratio than sneakers ads. But what is the exact probability that this is the case? To find out, you will have to calculate the posterior difference between clothes and sneakers click rates. Then, you will calculate a credible interval for the difference to measure the uncertainty in the estimate. Finally, you will calculate the percentage of cases where this difference is positive, which corresponds to clothes click rate being higher. Let's get on with it!\n",
    "\n",
    "arviz, seaborn, and matplotlib.pyplot have been imported for you as az, sns, and plt, respectively. Also, clothes_posterior and sneakers_posterior which you have calculated in the previous exercise are available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate posterior difference and plot it\n",
    "diff = clothes_posterior - sneakers_posterior\n",
    "sns.kdeplot(diff, shade=True, label=\"diff\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print 90% credible interval of posterior difference\n",
    "interval = az.hdi(diff, hdi_prob=0.9)\n",
    "print(interval)\n",
    "\n",
    "# Calculate and print probability of clothes ad being better\n",
    "clothes_better_prob = (diff > 0).mean()\n",
    "print(clothes_better_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ac5450",
   "metadata": {},
   "source": [
    "#### How bad can it be?\n",
    "You have concluded that with 98% probability, clothes ads have a higher click-through ratio than sneakers ads. This suggests rolling out the clothes campaign to a larger audience. However, there is a 2% risk that it's the sneakers ads that are actually better. If that's the case, how many clicks do we lose if we roll out the clothes campaign?\n",
    "\n",
    "The answer to this is the expected loss: the average posterior difference between the two click-through ratios given that sneakers ads do better. To calculate it, you only need to take the entries in the posterior difference where the sneakers click-through rate is higher and compute their average.\n",
    "\n",
    "The posterior difference between the click rates, diff, is available in your workspace. Let's find out how much is at risk!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba255af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice diff to take only cases where it is negative\n",
    "loss = diff[diff < 0]\n",
    "\n",
    "# Compute and print expected loss\n",
    "expected_loss = np.mean(loss)\n",
    "print(expected_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1c4ec9",
   "metadata": {},
   "source": [
    "#### Decision analysis: cost\n",
    "Your journey in marketing continues. You have already calculated the posterior click rates for clothes and sneakers ads, available in your workspace as clothes_posterior and sneakers_posteriors, respectively. Your boss, however, is not interested in the distributions of click rates. They would like to know what would be the cost of rolling out an ad campaign to 10'000 users. The company's advertising partner charges $2.5 per click on a mobile device and $2 on a desktop device. Your boss is interested in the cost of the campaign for each product (clothes and sneakers) on each platform (mobile and desktop): four quantities in total.\n",
    "\n",
    "Let's compare these four posterior costs using the forest plot from pymc3, which has been imported for you as pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14715402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distributions of the numbers of clicks for clothes and sneakers\n",
    "clothes_num_clicks = clothes_posterior * 10000\n",
    "sneakers_num_clicks = sneakers_posterior * 10000\n",
    "\n",
    "# Calculate cost distributions for each product and platform\n",
    "ads_costs = {\n",
    "    \"clothes_mobile\": clothes_num_clicks * 2.5,\n",
    "    \"sneakers_mobile\": sneakers_num_clicks * 2.5,\n",
    "    \"clothes_desktop\": clothes_num_clicks * 2,\n",
    "    \"sneakers_desktop\": sneakers_num_clicks * 2,\n",
    "}\n",
    "\n",
    "# Draw a forest plot of ads_costs\n",
    "pm.forestplot(ads_costs, hdi_prob=0.99, textsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd36b1c",
   "metadata": {},
   "source": [
    "#### Decision analysis: profit\n",
    "Good job translating the posterior click rates into cost distributions! In the meantime, a new company policy has been released. From now on, the goal of the marketing department is not to minimize the costs of campaigns, which was quite ineffective, but rather to maximize the profit. Can you adjust your findings accordingly, knowing that the expected revenue per click from a mobile ad is $3.4, and the one from a desktop ad is $3? To calculate the profit, you need to calculate the revenue from all clicks, then subtract the corresponding cost from it.\n",
    "\n",
    "Everything you have calculated in the previous exercise is available in your workspace: the ads_cost dictionary as well as the number of click distributions: clothes_num_clicks and sneakers_num_clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75251b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate profit distributions for each product and platform\n",
    "ads_profit = {\n",
    "    \"clothes_mobile\": clothes_num_clicks * 3.4 - ads_costs[\"clothes_mobile\"],\n",
    "    \"sneakers_mobile\": sneakers_num_clicks * 3.4 - ads_costs[\"sneakers_mobile\"],\n",
    "    \"clothes_desktop\": clothes_num_clicks * 3 - ads_costs[\"clothes_desktop\"],\n",
    "    \"sneakers_desktop\": sneakers_num_clicks * 3 - ads_costs[\"sneakers_desktop\"],\n",
    "}\n",
    "\n",
    "# Draw a forest plot of ads_profit\n",
    "pm.forestplot(ads_profit, hdi_prob=0.99)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c9b6bb",
   "metadata": {},
   "source": [
    "#### Analyzing regression parameters\n",
    "Your linear regression model has four parameters: the intercept, the impact of clothes ads, the impact of sneakers ads, and the variance. The draws from their respective posterior distributions have been sampled for you and are available as intercept_draws, clothes_draws, sneakers_draws, and sd_draws, respectively.\n",
    "\n",
    "Before you make predictions with your model, it's a good practice to analyze the posterior draws visually. In this exercise, you will first take a look at the descriptive statistics for each parameter's draws, and then you will visualize the posterior distribution for one of them as an example. pymc3 and pandas have been imported for you as pm and pd, respectively. Let's take a look at the parameters draws!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c24bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect parameter draws in a DataFrame\n",
    "posterior_draws_df = pd.DataFrame({\n",
    "    \"intercept_draws\": intercept_draws,\n",
    "    \"clothes_draws\": clothes_draws,\n",
    "  \t\"sneakers_draws\": sneakers_draws,\n",
    "    \"sd_draws\": sd_draws,\n",
    "})\n",
    "\n",
    "# Describe parameter posteriors\n",
    "draws_stats = posterior_draws_df.describe()\n",
    "print(draws_stats)\n",
    "\n",
    "# Plot clothes parameter posterior\n",
    "pm.plot_posterior(clothes_draws, hdi_prob=0.99)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24fb7b6",
   "metadata": {},
   "source": [
    "#### Predictive distribution\n",
    "Good job analyzing the parameter draws! Let's now use the linear regression model to make predictions. How many clicks can we expect if we decide to show 10 clothes ads and 10 sneaker ads? To find out, you will have to draw from the predictive distribution: a normal distribution with the mean defined by the linear regression formula and standard deviation estimated by the model.\n",
    "\n",
    "First, you will summarize each parameter's posterior with its mean. Then, you will calculate the mean of the predictive distribution according to the regression equation. Next, you will draw a sample from the predictive distribution and finally, you will plot its density. Here is the regression formula for your convenience:\n",
    "\n",
    "The number of clicks has a normal distribution with the mean β0 + β1 * clothes-ads-shown + β2 * sneakers-ads-shown, and some standard deviation sigma.\n",
    "\n",
    "pymc3, numpy, and seaborn have been imported under their usual aliases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba038c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate posteriors of the parameters to point estimates\n",
    "intercept_coef = np.mean(intercept_draws)\n",
    "sneakers_coef = np.mean(sneakers_draws)\n",
    "clothes_coef = np.mean(clothes_draws)\n",
    "sd_coef = np.mean(sd_draws)\n",
    "\n",
    "# Calculate the mean of the predictive distribution\n",
    "pred_mean = intercept_coef + sneakers_coef * 10 + clothes_coef * 10\n",
    "\n",
    "# Sample 1000 draws from the predictive distribution\n",
    "pred_draws = np.random.normal(pred_mean, sd_coef, size=1000)\n",
    "\n",
    "# Plot the density of the predictive distribution\n",
    "pm.plot_posterior(pred_draws, hdi_prob=0.99)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cd75f8",
   "metadata": {},
   "source": [
    "#### Inspecting posterior draws\n",
    "You continue working on your task to predict the number of bikes rented per day in a bike-sharing system. The posterior draws from your regression model which you sampled before are available in your workspace as trace_1.\n",
    "\n",
    "You know that after obtaining the posteriors, it is best practice to take a look at them to see if they make sense and if the MCMC process has converged successfully. In this exercise, you will create two plots visualizing posterior draws and summarize them in a table. Let's inspect our posteriors!\n",
    "\n",
    "NOTE: Please allow up to half a minute for the plots to render, since they have many draws to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855a4f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pymc3\n",
    "import pymc3 as pm\n",
    "\n",
    "# Draw a trace plot of trace_1\n",
    "pm.traceplot(trace_1)\n",
    "plt.show()\n",
    "\n",
    "# Draw a forest plot of trace_1\n",
    "pm.forestplot(trace_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e73279",
   "metadata": {},
   "source": [
    "#### Comparing models with WAIC\n",
    "Now that you have successfully built the first, basic model, you take another look at the data at your disposal. You notice a variable called wind_speed. This could be a great predictor of the numbers of bikes rented! Cycling against the wind is not that much fun, is it?\n",
    "\n",
    "You fit another model with this additional predictor:\n",
    "\n",
    "formula = \"num_bikes ~ temp + work_day + wind_speed\"\n",
    "\n",
    "with pm.Model() as model_2:\n",
    "    pm.GLM.from_formula(formula, data=bikes)\n",
    "    trace_2 = pm.sample(draws=1000, tune=500)\n",
    "Is your new model_2 better than model_1, the one without wind speed? Compare the two models using Widely Applicable Information Criterion, or WAIC, to find out!\n",
    "\n",
    "Both trace_1 and trace_2 are available in your workspace, and pycm3 has been imported as pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1059e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather trace_1 and trace_2 into a dictionary\n",
    "traces_dict = {\"trace_1\": trace_1, \"trace_2\": trace_2}\n",
    "\n",
    "# Create a comparison table based on WAIC\n",
    "comparison = pm.compare(traces_dict, ic=\"waic\")\n",
    "\n",
    "# Draw a comparison plot\n",
    "pm.compareplot(comparison, textsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40bf448",
   "metadata": {},
   "source": [
    "#### Sample from predictive density\n",
    "Finally! Your job is to predict the number of bikes rented per day, and you are almost there. You have fitted the model and verified the quality of parameter draws. You have also chosen the better of the two competing models based on the WAIC. Now, it's time to use your best model to make predictions!\n",
    "\n",
    "A couple of new observations, not seen by the model, have been collected in a DataFrame named bikes_test. For each of them, we know the true number of bikes rented, which will allow us to evaluate model performance. In this exercise, you will get familiar with the test data and generate predictive draws for every test observation. The trace of your model which you have generated before is available as trace_2, and pymc3 has been imported as pm. Let's make predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27fd6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print bikes_test head\n",
    "print(bikes_test.head())\n",
    "\n",
    "# Define the formula\n",
    "formula = \"num_bikes ~ temp + work_day + wind_speed\"\n",
    "\n",
    "# Generate predictive draws\n",
    "with pm.Model() as model:\n",
    "    pm.GLM.from_formula(formula, data=bikes_test)\n",
    "    posterior_predictive = pm.fast_sample_posterior_predictive(trace_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7048d092",
   "metadata": {},
   "source": [
    "#### Estimating test error\n",
    "Now that you have your posterior_predictive (available to you in your workspace), you can evaluate model performance on new data. To do this, you will need to loop over the test observations, and for each of them, compute the prediction error as the difference between the predictive distribution for this observation and the actual, true value. This will give you the distribution of your model's error, which you can then visualize.\n",
    "\n",
    "You will need pymc3 and numpy, which have been imported for you as pm and np, respectively. The test data, bikes_test, is also available in your workspace. Let's get to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56b532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize errors\n",
    "errors = []\n",
    "\n",
    "# Iterate over rows of bikes_test to compute error per row\n",
    "for index, test_example in bikes_test.iterrows():\n",
    "    error = posterior_predictive[\"y\"][:, index] - test_example[\"num_bikes\"]\n",
    "    errors.append(error)\n",
    "\n",
    "# Reshape errors\n",
    "error_distribution = np.array(errors).reshape(-1)\n",
    "\n",
    "# Plot the error distribution\n",
    "pm.plot_posterior(error_distribution)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db2717c",
   "metadata": {},
   "source": [
    "#### Inspecting the model\n",
    "Well done getting the model-building right! The trace is available in your workspace and, following the best practices, you will now inspect the posterior draws to see if there are any convergence issues. Next, you will extract each model parameter from the trace and summarize it with its posterior mean. These posterior means will come in handy later, when you will be making predictions with the model. Let's take a look at the parameter draws!\n",
    "\n",
    "You will need to use pymc3 and numpy, which have been imported for you as pm and np, respectively.\n",
    "\n",
    "NOTE: Please allow up to half a minute for the plots to render, since they have many draws to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e449bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a trace plot of trace\n",
    "pm.traceplot(trace)\n",
    "plt.show()\n",
    "\n",
    "# Print a summary of trace\n",
    "summary = pm.summary(trace)\n",
    "print(summary)\n",
    "\n",
    "# Extract each model parameter\n",
    "intercept_mean = np.mean(trace.get_values(\"Intercept\")) \n",
    "organic_mean = np.mean(trace.get_values(\"type_organic\")) \n",
    "price_mean = np.mean(trace.get_values(\"price\")) \n",
    "sd_mean = np.mean(trace.get_values(\"sd\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89070488",
   "metadata": {},
   "source": [
    "#### Optimizing the price\n",
    "Great job on fitting and inspecting the model! Now, down to business: your boss asks you to provide the avocado price that would yield the largest profit, and to state what profit can be expected. Also, they want the price to be divisible by $0.25 so that the customers can easily pay with quarters.\n",
    "\n",
    "In this exercise, you will use your model to predict the volume and the profit for a couple of sensible prices. Next, you will visualize the predictive distributions to pick the optimal price. Finally, you will compute the credible interval for your profit prediction. Now go and optimize!\n",
    "\n",
    "The posterior means you have computed before are available to you as intercept_mean, organic_mean, price_mean, and sd_mean, respectively. Also, pymc3, arviz, and numpy are imported as pm, az, and np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db9eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each price, predict volume and use it to predict profit\n",
    "predicted_profit_per_price = {}\n",
    "for price in [0.5, 0.75, 1, 1.25]:\n",
    "    pred_mean = (intercept_mean + price_mean * price + organic_mean)\n",
    "    volume_pred = np.random.normal(pred_mean, sd_mean, size=1000)\n",
    "    profit_pred = price * volume_pred\n",
    "    predicted_profit_per_price.update({price: profit_pred})\n",
    "    \n",
    "# Draw a forest plot of predicted profit for all prices\n",
    "pm.forestplot(predicted_profit_per_price)\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print HPD of predicted profit for the optimal price\n",
    "opt_hpd = az.hdi(predicted_profit_per_price[0.75], credible_interval=0.99)\n",
    "print(opt_hpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340690e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d91f21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a004c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ac78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8a183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
