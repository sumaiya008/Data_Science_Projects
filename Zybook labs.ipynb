{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9877919d",
   "metadata": {},
   "source": [
    "__1.10:  LAB: Importing modules__ \n",
    "\n",
    "Import the numpy and scipy modules with the aliases used in this material. These modules will be used with user input integers to create arrays and perform a linear regression. Linear regression will be covered in a different chapter.\n",
    "\n",
    "Ex: When the input is:\n",
    "\n",
    "1\n",
    "6\n",
    "12\n",
    "15\n",
    "The output is:\n",
    "\n",
    "LinregressResult(slope=1.3846153846153846, intercept=-1.2692307692307683, rvalue=0.8207293546575958, pvalue=0.17927064534240414, stderr=0.6815401979488211, intercept_stderr=6.866327235439774)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99635338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "6\n",
      "12\n",
      "15\n",
      "LinregressResult(slope=1.3846153846153846, intercept=-1.2692307692307683, rvalue=0.8207293546575958, pvalue=0.17927064534240414, stderr=0.6815401979488211, intercept_stderr=6.866327235439774)\n"
     ]
    }
   ],
   "source": [
    "# add code to import numpy and scipy\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "x1 = int(input())\n",
    "x2 = int(input())\n",
    "x3 = int(input())\n",
    "x4 = int(input())\n",
    "\n",
    "x = np.array([x1, x2, x3, x4])\n",
    "y = np.array([0, 10, 7, 25])\n",
    "\n",
    "print(st.linregress(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0cdc36",
   "metadata": {},
   "source": [
    "__1.11 LAB: Data frames__\n",
    "\n",
    "The Cars dataset has three columns giving the quality, machining angle, and machining speed of 18 cars.\n",
    "\n",
    "Write a program that performs the following tasks:\n",
    "\n",
    "Load the data Cars.csv into a data frame called cars_df.\n",
    "\n",
    "Subset the first userNum rows of the data frame into a new data frame.\n",
    "\n",
    "Find and print the maximum values of each column in the subset.\n",
    "\n",
    "Ex: If the input is:\n",
    "\n",
    "5\n",
    "the output is:\n",
    "\n",
    "Quality    5\n",
    "Speed      4\n",
    "Angle      3\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cars_df = pd.read_csv(\"Cars.csv\")# Import the CSV file Cars.csv\n",
    "userNum = int(input())\n",
    "subset = cars_df[0:userNum]\n",
    "print(subset.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb1f0bb",
   "metadata": {},
   "source": [
    "__1.12 LAB: Subsetting data frames__\n",
    "Data frames can be subset by a chosen value using ==.\n",
    "\n",
    "Ex: df[df.x == 5] gives all the rows in the data frame df for which the column x has a value of 5.\n",
    "\n",
    "Write a program that performs the following tasks:\n",
    "\n",
    "Load the file mtcars.csv into a data frame called df\n",
    "\n",
    "Create a new data frame df_cyl by subsetting the data frame based on a user defined value for the column cyl.\n",
    "\n",
    "Print the shape of the new data frame.\n",
    "\n",
    "Ex: If the input is:\n",
    "\n",
    "4\n",
    "the output is:\n",
    "\n",
    "(11, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253257c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cylinders = int(input())\n",
    "df = pd.read_csv('mtcars.csv') # read in the csv file mtcars.csv\n",
    " # create a new dataframe with only the rows where cyl = cylinders\n",
    "print(df[df.cyl==cylinders].shape) # print the shape of the new data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb3224",
   "metadata": {},
   "source": [
    "__1.13 LAB: Bar charts__\n",
    "\n",
    "Write a program that performs the following tasks:\n",
    "\n",
    "Load the pandas module.\n",
    "Load the dataset in titanic.csv as titanic.\n",
    "Create a new data frame, first_south, by subsetting titanic to include instances where a passenger is in the first class cabin (pclass column is 1) and boarded from Southampton (embarked column is S).\n",
    "Create a new data frame, second_third, by subsetting titanic to include instances where a passenger is either in the second (pclass column is 2) or third class (pclass column is 3) cabin.\n",
    "Using the template from this link, create the bar charts for the following:\n",
    "passengers in first class who embarked in Southampton grouped by sex\n",
    "passengers in second and third class grouped by survival status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af19c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pandas module as pd\n",
    "import pandas as pd\n",
    "\n",
    "titanic = pd.read_csv('titanic.csv')  # load titanic.csv\n",
    "\n",
    "first_south = titanic[(titanic.pclass == 1) & (titanic.embark_town == \"Southampton\")]  # subset the titanic dataset to include first class passengers who embarked in Southampton\n",
    "\n",
    "second_third = titanic[titanic.pclass == 3] # subset the titanic dataset to include either second or third class passengers\n",
    "\n",
    "print(first_south.head())\n",
    "print(second_third.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c49852",
   "metadata": {},
   "source": [
    "__1.14 LAB: Line charts__\n",
    "\n",
    "Write a program that subsets the loads and subsets the stock market data in target.csv.\n",
    "\n",
    "Load the the data from target.csv as target.\n",
    "Create a new data frame, tgt_march, by subsetting the last 19 days of the target data frame.\n",
    "Create a new data frame, tgt_vol, by subsetting the Date and Volume columns.\n",
    "Create a new data frame, tgt_close, by subsetting the Date and Close columns.\n",
    "Using the template from this link, create separate line charts for Volume and Close and upload the resulting output.\n",
    "If the input is:\n",
    "\n",
    "3\n",
    "The output is:\n",
    "\n",
    "The volume of TGT on 2018-03-05 is 7654766.\n",
    "The closing stock price of TGT on 2018-03-05 is $75.14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f325beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pandas module\n",
    "import pandas as pd\n",
    "\n",
    "tgt = pd.read_csv(\"target.csv\")  # load the target.csv file\n",
    "\n",
    "tgt_march = tgt[-19:] # subset the last 19 days of the dataframe\n",
    "\n",
    "tgt_vol = tgt_march[[\"Date\",\"Volume\"]] # subset tgt_march and create a data frame that contains the columns: Date and Volume\n",
    "\n",
    "tgt_close = tgt_march[[\"Date\",\"Close\"]] # subset tgt_march and create a data frame that contains the columns: Date and Closing\n",
    "\n",
    "day = int(input()) - 1\n",
    "inputDay = day + 1\n",
    "\n",
    "volume_row =  tgt_vol[day:inputDay]  # subset the specific row of tgt_vol for the given day\n",
    "volume = volume_row.iloc[0][1]  # gets the volume for the given day\n",
    "\n",
    "close_row = tgt_close[day:inputDay]  # subset the specific row of tgt_close for the given day\n",
    "close = close_row.iloc[0][1]  # gets the closing stock price for the given day\n",
    "\n",
    "date = tgt_march.iloc[[day]].iloc[0][0] # gets the date\n",
    "\n",
    "print(\"The volume of TGT on \" + str(date) + \" is \" + str(int(volume)) + \".\")\n",
    "print(\"The closing stock price of TGT on \" + str(date) + \" is $\" + str(close) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596202f",
   "metadata": {},
   "source": [
    "__1.15 LAB: Strip plots__\n",
    "\n",
    "Write a program that loads and subsets the dataset in titanic.csv.\n",
    "\n",
    "Load the pandas module.\n",
    "Load the dataset in titanic.csv as titanic.\n",
    "Create a new data frame, df, by subsetting the titanic data frame to include instances with male first class passengers who are over 18 years old.\n",
    "Using the template from this link, create and upload a strip plot where the data is grouped by the city the passengers embarked and by survival status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pandas module as pd\n",
    "import pandas as pd\n",
    "\n",
    "titanic = pd.read_csv(\"titanic.csv\") # load the titanic.csv dataset\n",
    "\n",
    "df = titanic[(titanic.sex == \"male\") & (titanic.age > 18) & (titanic.pclass == 1)]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0fc1d9",
   "metadata": {},
   "source": [
    "__2.6 LAB: Measures of center__\n",
    "\n",
    "Given main.py, complete the program that performs the following tasks:\n",
    "\n",
    "Import the csv file mtcars.csv as a data frame using a pandas module function.\n",
    "\n",
    "Find the mean, median, and mode of the column wt.\n",
    "\n",
    "Print the mean, median, and mode\n",
    "\n",
    "Ex: for the column qsec, the output would be:\n",
    "\n",
    "mean = 17.84875, median = 17.710, mode = 0    17.02\n",
    "1    18.90\n",
    "Name: qsec, dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c59fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"mtcars.csv\") # read in the file mtcars.csv\n",
    "\n",
    "mean = df['wt'].mean() # find the mean of the column wt\n",
    "median = df['wt'].median() # find the median of the column wt\n",
    "mode = df['wt'].mode() # find the mode of the column wt\n",
    "\n",
    "print(\"mean = {:.5f}, median = {:.3f}, mode = {}\".format(mean, median, mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3332d1ee",
   "metadata": {},
   "source": [
    "__2.7 LAB: Standard deviation__\n",
    "\n",
    "Descriptive statistics are important in sports. Data often involves a large number of measurements and players.\n",
    "\n",
    "The NBA2019 dataset was taken from nbastuffer and includes information on several players such as team name, age, turnover percentage, and points per game.\n",
    "\n",
    "Write a program to find the sample standard deviation, rounded to two decimal places, for all players on the list in a chosen column.\n",
    "\n",
    "Ex: If the input is:\n",
    "\n",
    "PointsPerGame\n",
    "Then the output is:\n",
    "\n",
    "The standard deviation for PointsPerGame is: 2.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b6e2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Also import the scipy.stats module.\n",
    "import scipy.stats as st\n",
    "\n",
    "NBA2019_df = pd.read_csv(\"NBA2019.csv\")  #'''Type your code here to load the csv file NBA2019.csv.'''\n",
    "\n",
    "# Input desired column. Ex: AGE, 2P%, or PointsPerGame.\n",
    "chosen_column = str(input())  #'''Complete input code here.'''\n",
    "\n",
    "\n",
    "# Create subset of NBA2019_df based on input.\n",
    "NBA2019_df_column = NBA2019_df[chosen_column] #'''Type your code here to subset NBA2019_df based on the chosen column.'''\n",
    "\n",
    "\n",
    "# Find standard deviation and round to two decimal places. \n",
    "sample_s = st.tstd(NBA2019_df_column)\n",
    "sample_s_rounded = round(sample_s, 2)  #The student has incorrectly used the round() function.\n",
    "\n",
    "\n",
    "# Output\n",
    "print('The standard deviation for ' + chosen_column + ' is:', sample_s_rounded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370f5dc",
   "metadata": {},
   "source": [
    "__2.8 LAB: Five number summary__\n",
    "\n",
    "Write a program that will do the following tasks:\n",
    "\n",
    "Load the file internetusage.csv into a data frame called df.\n",
    "Create a new data frame, internet, by subsetting the internet_usage column.\n",
    "Print the five number summary for the internet data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12804691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"internetusage.csv\")\n",
    "\n",
    "internet = df[[\"internet_usage\"]] # subset the column internet_usage\n",
    "\n",
    "five_num = internet.describe()  # find the five number summary\n",
    "\n",
    "print(five_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40fa449",
   "metadata": {},
   "source": [
    "__4.11 LAB: Poisson distribution__\n",
    "    \n",
    "Generate a Poisson distribution r1 of 8 numbers with a user input mean, lam.\n",
    "Generate a Poisson distribution r2 of 20 numbers with a mean of lam.\n",
    "Generate a Poisson distribution r3 of 100 numbers with a mean of lam.\n",
    "Find the cumulative probability of user defined integer x or fewer successes for a Poisson distribution with a mean of lam\n",
    "Find the theoretical mean of the generated Poisson distributions\n",
    "Calculate the actual means of r1, r2, and r3\n",
    "Ex: When the input is:\n",
    "\n",
    "2\n",
    "5\n",
    "the output is:\n",
    "\n",
    "Cumulative probability is  0.12465201948308108\n",
    "Theoretical mean is  5.0\n",
    "Mean of r1 is  6.0\n",
    "Mean of r2 is  5.35\n",
    "Mean of r3 is  5.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2531e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# import the correct module and function\n",
    "from scipy.stats import poisson\n",
    "\n",
    "x = int(input())\n",
    "lam = int(input())\n",
    "\n",
    "np.random.seed(seed=123)\n",
    "\n",
    "# Generate 8 random numbers with a Poisson distribution with a mean of lam\n",
    "r1 = poisson.rvs(lam, size=8) # Code for generating distribution\n",
    "\n",
    "# Generate 20 random numbers with a Poisson distribution with a mean of lam\n",
    "r2 = poisson.rvs(lam, size=20) # Code for generating distribution\n",
    "\n",
    "# Generate 100 random numbers with a Poisson distribution with a mean of lam\n",
    "r3 = poisson.rvs(lam, size=100)# Code for generating distribution\n",
    "\n",
    "# Calculate the cumulative probability of x or fewer successes for a Poisson distribution with a mean of lam\n",
    "cp = poisson.cdf(x, lam) # Code for cumulative probability\n",
    "print(\"Cumulative probability is \", cp)\n",
    "\n",
    "# Calculate the theoretical mean of the Poisson distribution\n",
    "mean_theor = poisson.stats(lam, moments='m') # Code for theoretical mean\n",
    "print(\"Theoretical mean is \", mean_theor)\n",
    "\n",
    "# Calculate the actual mean of r1, r2, and r3\n",
    "mean_r1 = np.mean(r1)  # Code for actual mean of r1\n",
    "print(\"Mean of r1 is \", mean_r1)\n",
    "\n",
    "mean_r2 = np.mean(r2) # Code for actual mean of r2\n",
    "print(\"Mean of r2 is \", mean_r2)\n",
    "\n",
    "mean_r3 = np.mean(r3) # Code for actual mean of r3\n",
    "print(\"Mean of r3 is \", mean_r3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83219506",
   "metadata": {},
   "source": [
    "__4.12 LAB: Normal distribution__\n",
    "\n",
    "The normalized distribution for a person's intelligence quotient (IQ) has a mean of 100 and a standard deviation of 15. Write a program that takes two values for IQ (IQ1 and IQ2) as inputs and calculates the following probabilities for a randomly selected person having an IQ:\n",
    "\n",
    "greater than or equal to IQ1\n",
    "less than or equal to IQ1\n",
    "between IQ1 and IQ2\n",
    "For example, if the input is:\n",
    "\n",
    "100\n",
    "105\n",
    "the output is:\n",
    "\n",
    "The probability that a randomly selected person \n",
    " has an IQ of at least 100.0 is 0.5.\n",
    "The probability that a randomly selected person \n",
    " has an IQ of at least 100.0 is 0.5.\n",
    "The probability that a randomly selected person \n",
    " has an IQ of between 100.0 and 105.0 is 0.13055865981823633."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8f1275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.stats\n",
    "import scipy.stats as st\n",
    "\n",
    "# input two IQs, making sure that IQ1 is less than IQ2\n",
    "IQ1 = float(input())\n",
    "IQ2 = float(input())\n",
    "\n",
    "while IQ1 > IQ2:\n",
    "    print(\"IQ1 should be less than IQ2. Enter numbers again.\")\n",
    "    IQ1 = float(input())\n",
    "    IQ2 = float(input())\n",
    "\n",
    "prob_al = st.norm.sf(IQ1, 100, 15) # write a command that calculates the probability that a randomly\n",
    "          # selected person has is greater than or equal to IQ1.\n",
    "prob_am = st.norm.cdf(IQ1, 100, 15) # write a command that calculates the probability that a randomly\n",
    "          # selected person has is less than or equal to IQ1.\n",
    "\n",
    "prob_bet = st.norm.cdf(IQ2, 100, 15) - st.norm.cdf(IQ1, 100, 15) # write command that calculates the probability that a randomly\n",
    "           # selected person has is between IQ1 and IQ2\n",
    "\n",
    "print(\"The probability that a randomly selected person \\n has an IQ of at least \" + str(IQ1) + \" is \" + str(prob_al) + \".\")\n",
    "print(\"The probability that a randomly selected person \\n has an IQ of at least \" + str(IQ1) + \" is \" + str(prob_am) + \".\")\n",
    "print(\"The probability that a randomly selected person \\n has an IQ of between \" + str(IQ1) + \" and \" + str(IQ2)+ \" is \" + str(prob_bet) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b04cc19",
   "metadata": {},
   "source": [
    "__4.13 LAB: Student's t-distribution__\n",
    "    \n",
    "The United States Census Bureau determined that the mean number of children in an American household is 1.86. Suppose 50 households are polled and the sample mean is found to be 2.1 and the standard deviation is found to be 1.57.\n",
    "\n",
    "Write a program that finds the probability that another 50 household sample will have a sample mean of at least 2.1.\n",
    "\n",
    "The output should be:\n",
    "\n",
    "The probability that another 50 household sample will have a sample mean of at least 2.1 is 0.14251039487167508."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fec4ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import math\n",
    "# the math package is necessary because the expression for t uses the math.sqrt(x) function.\n",
    "\n",
    "n = 50 # what is the sample size?\n",
    "pop_mean = 1.86 # what is the population mean?\n",
    "sam_mean = 2.1 # what is the sample mean?\n",
    "sd = 1.57 # what is the standard deviation?\n",
    "df = n-1 # what is the degrees of freedom?\n",
    "\n",
    "x = math.sqrt(n)\n",
    "t = (sam_mean - pop_mean)/(sd/x) # write an expression to find the value of t. \n",
    "\n",
    "prob = st.t.sf(t, df) # use the st.t.sf function to find the probability\n",
    "\n",
    "\n",
    "print(\"The probability that another 50 household sample will have a sample mean of at least 2.1 is \" + str(prob) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5857ef4c",
   "metadata": {},
   "source": [
    "__4.14 LAB: Binomial distribution__\n",
    "\n",
    "Given a fair coin flipped 100 times, find the probability of a user-defined number of flips coming up heads.\n",
    "Find the mean and variance of the binomial distribution.\n",
    "Ex: When the input is:\n",
    "\n",
    "27\n",
    "the output is:\n",
    "\n",
    "1.51252E-06\n",
    "50.00\n",
    "25.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c2adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary module\n",
    "from scipy.stats import binom\n",
    "\n",
    "# Get user-defined number of successes\n",
    "k = int(input())\n",
    "\n",
    "# Set the number of trials and the probability of success in each trial\n",
    "\n",
    "# Calculate the probability of k successes given the defined n and p\n",
    "P = binom.pmf(k, 100, 0.5) # Code to calculate probability\n",
    "print(f'{P:.5E}')\n",
    "\n",
    "# Return the mean of the distribution\n",
    "mean  = binom.stats(100, 0.5, moments = 'm')  # Code to find mean\n",
    "print(f'{mean:.2f}')\n",
    "\n",
    "# Return the variance of the distribution \n",
    "var = binom.stats(100, 0.5, moments='v') # Code to find variance\n",
    "print(f'{var:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f31284",
   "metadata": {},
   "source": [
    "__4.15 LAB: Hypergeometric distribution__\n",
    "\n",
    "Given user defined numbers k and n, if n cards are drawn from a deck, find the probability that k cards are black.\n",
    "Find the probability that at least k cards are black.\n",
    "Ex: When the input is:\n",
    "\n",
    "11\n",
    "7\n",
    "the output is:\n",
    "\n",
    "0.162806\n",
    "0.249278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary module\n",
    "from scipy.stats import hypergeom\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = int(input())\n",
    "k = int(input())\n",
    "# Define N and x\n",
    "N = 52\n",
    "x = 26\n",
    "\n",
    "# Calculate the probability of k successes given the defined N, x, and n\n",
    "P = hypergeom.pmf(k, N, x, n, loc=0) # Code to calculate probability\n",
    "print(f'{P:.6f}')\n",
    "\n",
    "# Calculate the cumulative probability of k or more successes\n",
    "cp = 1 - hypergeom.cdf(k, N, x, n) + P # Code to calculate cumulative probability\n",
    "\n",
    "print(f'{cp:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0314256b",
   "metadata": {},
   "source": [
    "__5.10 LAB: Confidence intervals for population means__\n",
    "    \n",
    "norm.rsv is used to generate 100 random values with a normal distribution with a user defined mean and standard deviation.\n",
    "Calculate the sample mean of the distribution.\n",
    "Calculate the standard error of the distribution.\n",
    "Calculate the 95% confidence interval for a normal distribution with the sample mean and standard error.\n",
    "Determine if the user defined mean falls within the confidence interval.\n",
    "Ex: When the input is:\n",
    "\n",
    "0\n",
    "5\n",
    "the output is:\n",
    "\n",
    "Sample mean is 0.13554536745179882\n",
    "(-0.9701099765668659, 1.2412007114704635)\n",
    "User defined mean, 0 , is within the 95% confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dc19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary modules\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "\n",
    "# Get user defined mean and standard deviation\n",
    "mean_def = int(input())\n",
    "sd_def = int(input())\n",
    "\n",
    "np.random.seed(seed=123)\n",
    "\n",
    "# Generate 100 random numbers with a normal distribution with a mean of mean_def and a standard deviation of sd_def\n",
    "r = st.norm.rvs(mean_def, sd_def, size=100)\n",
    "\n",
    "# Calculate the sample mean of r\n",
    "mean = r.mean()  # Code for calculating mean\n",
    "print(\"Sample mean is\", mean)\n",
    "\n",
    "# Calculate the standard error of r\n",
    "std_dev = r.std()  # Code for calculating standard deviation of r\n",
    "stderr = std_dev/(100**0.5)  # Code for calculating standard error\n",
    "\n",
    "# Calculate the 95% confidence interval using the sample mean and standard error\n",
    "int1 = st.norm.interval(0.95, mean, stderr) # Code for confidence interval\n",
    "print(int1)\n",
    "\n",
    "# Determine if the user defined mean falls within the 95% confidence interval\n",
    "if mean not in int1:  # Finish the if statement\n",
    "    print(\"User defined mean,\", mean_def,\", is within the 95% confidence interval\")\n",
    "else:\n",
    "   print(\"User defined mean,\", mean_def,\", is not within the 95% confidence interval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36f2e7c",
   "metadata": {},
   "source": [
    "__5.11 LAB: Confidence intervals for population proportions__\n",
    "\n",
    "Load the titanic dataset.\n",
    "Find the proportion of passengers who embarked from Southampton whose age is greater than the user defined age.\n",
    "Calculate the 95% confidence interval for the sample proportion.\n",
    "Find proportion of all passengers whose age is greater than the user defined age.\n",
    "Report whether the actual proportion is within the sample confidence interval.\n",
    "When the input is:\n",
    "\n",
    "50\n",
    "the output is:\n",
    "\n",
    "Sample proportion is 0.06987577639751552\n",
    "(0.05018608279208029, 0.08956547000295076)\n",
    "Actual proportion, 0.0718294051627385 , is within the 95% confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7f106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "age = int(input())\n",
    "\n",
    "# Read csv file titanic.csv into data frame\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Take the subset of the data where Embarked = \"S\"\n",
    "south = df[df[\"Embarked\"] ==\"S\"] # Code to take subset\n",
    "\n",
    "# Find total number in subset\n",
    "n1 =south.shape[0] # Code to count number in subset\n",
    "\n",
    "# Find number in subset where Age > age\n",
    "x1 = len(south[south[\"Age\"]>age])  # Code to count number where Age > age\n",
    "\n",
    "# Calculate proportion\n",
    "p1 = x1/n1 # Code to calculate proportion \n",
    "print(\"Sample proportion is\", p1)\n",
    "\n",
    "# Calculate standard error\n",
    "stderr = (x1*(n1-x1)/(n1**3))**0.5 # Code to calculate standard error\n",
    "\n",
    "# Find 95% confidence interval\n",
    "z=st.norm.ppf(q=0.975)\n",
    "conf_int = (p1 - (z * stderr)+0.00000000000000001, p1 + (z *stderr))  # Code to find confidence interval\n",
    "print(conf_int)\n",
    "\n",
    "# Find proportion for full data\n",
    "n2 = len(df) # Code to count number in data set\n",
    "x2 = len(df[df[\"Age\"]>age]) # Code to count number in full data set where Age > age\n",
    "p2 = x2/n2  # Code to calculate proportion\n",
    "\n",
    "# Determine if the actual proportion falls within the 95% confidence interval\n",
    "if conf_int[0] <= p2 <= conf_int[1]:\n",
    "    print(\"Actual proportion,\", str(p2),\", is within the 95% confidence interval\")\n",
    "else:\n",
    "   print(\"Actual proportion,\", str(p2) ,\", is not within the 95% confidence interval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a732a167",
   "metadata": {},
   "source": [
    "__5.12 LAB: One-sample hypothesis test for population proportion__\n",
    "Load the gpa.csv data set.\n",
    "Find the number of students with a gpa greater than the user input value.\n",
    "Find the total number of students.\n",
    "Perform a z-test for the user input expected proportion.\n",
    "Determine if the hypothesis that the actual proportion is different from the expected proportion should be rejected at the alpha = 0.01 significance level.\n",
    "Ex: When the input is:\n",
    "\n",
    "0.5\n",
    "3.0\n",
    "the ouput is:\n",
    "\n",
    "(-0.16909988539863083, 0.8657180750703445)\n",
    "The two-tailed p-value, 0.8657180750703445 , is greater than than α. Thus, insufficient evidence exists to support the hypothesis that the proportion is different from 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e8c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the correct modules\n",
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "\n",
    "# Read in gpa.csv\n",
    "gpa = pd.read_csv('gpa.csv')  # Code to read in data set\n",
    "\n",
    "value = float(input())\n",
    "cutoff = float(input())\n",
    "\n",
    "gpacolumn = gpa['gpa']\n",
    "\n",
    "# Determine the number of students with a gpa higher than cutoff\n",
    "counts = sum(gpa['gpa']>cutoff)  # Code to count number of students with gpa > cutoff\n",
    "\n",
    "# Determine the total number of students\n",
    "nobs = len(gpa) # Code to count total number of students\n",
    "\n",
    "# Perform z-test for counts, nobs, and value\n",
    "ztest = proportions_ztest(counts, nobs, value)  # Code to perform z-test\n",
    "print(ztest)\n",
    "alpha = 0.01\n",
    "\n",
    "# Determine the correct conclusion\n",
    "if ztest[1]<alpha:  # Finish the if statment\n",
    "    print(\"The two-tailed p-value,\", ztest[1],\", is less than \\u03B1. Thus, sufficient evidence exists to support the hypothesis that the proportion is different from\", value)\n",
    "else:\n",
    "    print(\"The two-tailed p-value,\", ztest[1],\", is greater than than \\u03B1. Thus, insufficient evidence exists to support the hypothesis that the proportion is different from\", value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0d7570",
   "metadata": {},
   "source": [
    "__5.13 LAB: Two-sample t-test__\n",
    "\n",
    "Use the titanic.csv data set.\n",
    "Subset the data set into male and female.\n",
    "Choose the correct t-test to use to determine if male and female passengers paid the same mean fare.\n",
    "Ex: If the column SibSp is used instead of Fare, the output is:\n",
    "\n",
    "Ttest_indResult(statistic=-3.4405234245346836, pvalue=0.0006076214735498975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Read in the titanic.csv data set\n",
    "titanic = pd.read_csv('titanic.csv')  # Code to read in data\n",
    "\n",
    "# Subset the data into male and female\n",
    "\n",
    "male = titanic['Fare'][titanic['Sex'] == 'male']  # Code to subset on Sex = \"male\"\n",
    "female = titanic['Fare'][titanic['Sex'] == 'female']  # Code to subset on Sex = \"female\"\n",
    "\n",
    "# Use the correct t-test to compare values of Fare for both subsets\n",
    "t_test_results = st.ttest_ind(male,female)\n",
    "print(f'Ttest_indResult(statistic={t_test_results[0]:},pvalue={t_test_results[1]:})')# Code to perform t-test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64d73f1",
   "metadata": {},
   "source": [
    "__6.11 LAB: Creating SLR models__\n",
    "\n",
    "The nbaallelo_slr.csv dataset contains information on 126315 NBA games between 1947 and 2015. The columns report the points made by one team, the Elo rating of that team coming into the game, the Elo rating of the team after the game, and the points made by the opposing team.\n",
    "\n",
    "Load the data set into a data frame.\n",
    "Create a new column y in the data frame that is the difference between the points made by the two teams.\n",
    "Use the ols function to perform a simple linear regression on the y and elo_i columns.\n",
    "Create an analysis of variance table using the results of the SLR.\n",
    "Ex: If the Elo rating of the team after the game, elo_n, is used instead of elo_i, the output is:\n",
    "\n",
    "                sum_sq        df             F  PR(>F)\n",
    "elo_n     2.498800e+06       1.0  15705.983944     0.0\n",
    "Residual  2.009606e+07  126312.0           NaN     NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c65830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import pandas as pd\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "\n",
    "nba = pd.read_csv('nbaallelo_slr.csv') # Code to read in nbaallelo_slr.csv\n",
    "\n",
    "# Create a new column in the data frame that is the difference between pts and opp_pts\n",
    "nba['y'] = nba['pts'] - nba['opp_pts'] # Code to find the difference between the columns pts and opp_pts\n",
    "\n",
    "# Perform simple linear regression on y and elo_i\n",
    "results = ols('y ~ elo_i', data=nba).fit()  # Code to perform SLR using statsmodels ols \n",
    "# Create an analysis of variance table\n",
    "aov_table = sm.stats.anova_lm(results, typ=2)\n",
    " # Code to create ANOVA table\n",
    "\n",
    "# Print the analysis of variance table\n",
    "print(aov_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ab277f",
   "metadata": {},
   "source": [
    "__6.12 Lab: Making predictions using SLR models__\n",
    "\n",
    "The internetusage.csv dataset contains different statistics for all 50 states in the United States. Among those statistics are percentage of internet users in the state and percentage of individuals with bachelor's degrees.\n",
    "\n",
    "Write a program that creating a model that takes in the percentage of individuals with bachelor's degrees as input and returns the percentage of internet users in a state as output.\n",
    "\n",
    "For example, if the input is:\n",
    "\n",
    "10\n",
    "the output should be:\n",
    "\n",
    "0    61.865155\n",
    "dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0993ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sms\n",
    "\n",
    "internet = pd.read_csv(\"internetusage.csv\")\n",
    "internet = internet.rename(columns={\"internet_usage\": \"y\", \"bachelors_degree\": \"x\"}) # load the file internetusage.csv\n",
    "\n",
    "model = sms.ols('y~x', data = internet).fit() # fit a linear model using the sms.ols function and the internet dataframe\n",
    "\n",
    "bach_percent = float(input())\n",
    "\n",
    "prediction = model.predict(exog=dict(x=[bach_percent])) # use the model.predict function to find the predicted value for internet_usage using \n",
    "             # the bach_percent value for the predictor\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8578d9d2",
   "metadata": {},
   "source": [
    "__6.13 LAB: Creating correlation matrices__\n",
    "\n",
    "The nbaallelo_slr.csv data base contains information on 126315 NBA games between 1947 and 2015. The columns report the points made by one team, the Elo rating of that team coming into the game, the Elo rating of the team after the game, and the points made by the opposing team.\n",
    "\n",
    "Load the data set into a data frame.\n",
    "Find the correlation matrix for all three columns.\n",
    "Create a new column y in the data frame that is the difference between the points made by the two teams.\n",
    "Find the correlation matrix for y and the Elo rating column elo_i.\n",
    "Ex: If the Elo rating of the team after the game, elo_n, is used instead of elo_i, the output is:\n",
    "                    elo_n       pts   opp_pts\n",
    "elo_n    1.000000  0.121670 -0.178553\n",
    "pts      0.121670  1.000000  0.592491\n",
    "opp_pts -0.178553  0.592491  1.000000\n",
    "          elo_n         y\n",
    "elo_n  1.000000  0.332553\n",
    "y      0.332553  1.000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd7ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import pandas as pd\n",
    "\n",
    "nba = pd.read_csv('nbaallelo_slr.csv') # Code to read in nbaallelo_slr.csv\n",
    "\n",
    "# Display the correlation matrix for the columns elo_i, pts, and opp_pts\n",
    "print(nba[[\"elo_i\",\"pts\", \"opp_pts\"]].corr()) # Code to calculate correlation matrix)\n",
    "\n",
    "# Create a new column in the data frame that is the difference between pts and opp_pts\n",
    "nba['y'] = nba[\"pts\"] - nba[\"opp_pts\"] # Code to find the difference between the columns pts and opp_pts\n",
    "\n",
    "# Display the correlation matrix for elo_i and y\n",
    "print(nba[[\"elo_i\",\"y\"]].corr()) # Code to calculate the correlation matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c03712",
   "metadata": {},
   "source": [
    "__6.14 LAB: Multiple regression__\n",
    "\n",
    "The nbaallelo_slr.csv data base contains information on 126315 NBA games between 1947 and 2015. The columns report the points made by one team, the Elo rating of that team coming into the game, the Elo rating of the team after the game, and the points made by the opposing team.\n",
    "\n",
    "Load the data set into a data frame.\n",
    "Use the ols function to perform a multiple linear regression with pts as the response variable and opp_pts and elo_i as the predictor variables.\n",
    "Create an analysis of variance table using the results of the multiple regression.\n",
    "Ex: If the Elo rating of the team after the game, elo_n, is used instead of elo_i, the output is:\n",
    "\n",
    "                sum_sq        df             F  PR(>F)\n",
    "elo_n     1.481593e+06       1.0  11335.413055     0.0\n",
    "opp_pts   1.080327e+07       1.0  82653.915172     0.0\n",
    "Residual  1.650946e+07  126311.0           NaN     NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import pandas as pd\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "\n",
    "nba = pd.read_csv(\"nbaallelo_slr.csv\") # Code to read in nbaallelo_slr.csv\n",
    "\n",
    "\n",
    "# Perform multiple linear regression on pts, elo_i, and opp_pts\n",
    "results = ols(\"pts ~ elo_i + opp_pts\", data = nba).fit() # Code to perform multiple regression using statsmodels ols \n",
    "\n",
    "# Create an analysis of variance table\n",
    "aov_table = sm.stats.anova_lm(results, typ = 2) # Code to create ANOVA table\n",
    "\n",
    "# Print the analysis of variance table\n",
    "print(aov_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad9f77c",
   "metadata": {},
   "source": [
    "__6.15 LAB: Regression with categorical predictors__\n",
    "\n",
    "The nbaallelo_mlr.csv data base contains information on 126315 NBA games between 1947 and 2015. The columns report the points made by one team, the Elo rating of that team coming into the game, the Elo rating of the team after the game, whether the team won or lost, and the points made by the opposing team.\n",
    "\n",
    "Load the data set into a data frame.\n",
    "Recode the categorical variable game_result into a dummy variable with prefix game_result.\n",
    "Use the ols function to perform a multiple regression with pts as the response variable and elo_i, game_result_W, and opp_pts, in that order, as the predictor variables.\n",
    "Create an analysis of variance table using the results of the multiple regression.\n",
    "Ex: If the Elo rating of the team after the game, elo_n, is used instead of elo_i, the output is:\n",
    "\n",
    "                     sum_sq        df              F  PR(>F)\n",
    "elo_n          1.384812e+05       1.0    2440.029440     0.0\n",
    "game_result_W  9.340871e+06       1.0  164585.471121     0.0\n",
    "opp_pts        1.689391e+07       1.0  297669.387214     0.0\n",
    "Residual       7.168588e+06  126310.0            NaN     NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9236d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import pandas as pd\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "\n",
    "nba = pd.read_csv(\"nbaallelo_mlr.csv\") # Code to read in nbaallelo_mlr.csv\n",
    "\n",
    "# Recode the column game_result into a dummy variable, win, with W = 1 and L = 0\n",
    "nba = pd.get_dummies(nba, columns =[\"game_result\"])\n",
    "\n",
    "# Perform multiple linear regression with pts as the response variable and elo_i, win, ans opp_pts \n",
    "# in that order, as the predictor variables\n",
    "\n",
    "results = ols(\"pts ~ elo_i + game_result_W + opp_pts\", data = nba).fit()\n",
    "\n",
    "# Code to perform multiple regression using statsmodels ols \n",
    "# Create an analysis of variance table\n",
    "aov_table = sm.stats.anova_lm(results, typ = 2) # Code to create ANOVA table\n",
    "\n",
    "# Print the analysis of variance table\n",
    "print(aov_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ce0a6e",
   "metadata": {},
   "source": [
    "__6.16 LAB: Regression with nonlinear relationships__\n",
    "\n",
    "Load the mtcars.csv data set into a data frame.\n",
    "Create a new column in the data frame, exp_wt, that is the negative exponentiation of of the column wt.\n",
    "Use the ols function to perform a multiple regression with mpg as the response variable and wt and exp_wt, in that order, as the predictor variables.\n",
    "Create an analysis of variance table using the results of the multiple regression.\n",
    "Ex: If the column qsec is used as the response variable instead of mpg, the output is:\n",
    "\n",
    "             sum_sq    df         F    PR(>F)\n",
    "wt         0.397038   1.0  0.120078  0.731452\n",
    "exp_wt     0.077859   1.0  0.023547  0.879105\n",
    "Residual  95.888614  29.0       NaN       NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cce312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "cars = pd.read_csv('mtcars.csv') # Code to read in mtcars.csv\n",
    "\n",
    "# Create a new column in the data frame called exp_wt which is the negative exponentiation of the column wt\n",
    "cars['exp_wt'] = np.exp(-1*cars['wt'])\n",
    "\n",
    "# Perform multiple linear regression with mpg as the response variable and wt and exp_wt \n",
    "# in that order, as the predictor variables\n",
    "results = ols('mpg ~ wt + exp_wt', data=cars).fit() # Code to perform multiple regression using statsmodels ols \n",
    "\n",
    "# Create an analysis of variance table\n",
    "aov_table = sm.stats.anova_lm(results, typ=2)  # Code to create ANOVA table\n",
    "\n",
    "# Print the analysis of variance table\n",
    "print(aov_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ddd2c",
   "metadata": {},
   "source": [
    "__7.6 LAB: Chi-square test__\n",
    "\n",
    "The table below gives the frequency with which a sample of women drink tea and coffee (Arab, Lenore et al. \"Gender differences in tea, coffee, and cognitive decline in the elderly: the Cardiovascular Health Study.\" Journal of Alzheimer's disease : JAD vol. 27,3 (2011): 553-66. doi:10.3233/JAD-2011-110431). Do women drink tea and coffee with the same frequency at the alpha = 0.01 significance level?\n",
    "\n",
    "Freq\tTea\tCoffee\n",
    "< 5 / year\t580\t1019\n",
    "5 - 10 / year\t289\t169\n",
    "1 - 3 / month\t503\t178\n",
    "1 - 4 / week\t618\t208\n",
    "> 5 / week\t742\t1148\n",
    "Enter the data into an array.\n",
    "Calculate the chi-square statistic and p-value\n",
    "Write an if statement to determine if the null hypothesis that women drink tea and coffee with the same frequency is rejected or not.\n",
    "Ex: If the data for men is used,\n",
    "\n",
    "Freq\tTea\tCoffee\n",
    "< 5 / year\t551\t637\n",
    "5 - 10 / year\t244\t139\n",
    "1 - 3 / month\t387\t152\n",
    "1 - 4 / week\t452\t225\n",
    "> 5 / week\t443\t915\n",
    "the output is:\n",
    "\n",
    "377.6187409281143\n",
    "1.9030487979418984e-80\n",
    "The hypothesis that tea and coffee are drunk with the same frequency is rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c681264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from scipy import stats\n",
    "from scipy.stats import fisher_exact\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Construct a contingency table\n",
    "drink = np.array([[580, 1019],[289,169],[503,178],[618,208],[742,1148]])\n",
    "\n",
    "# Calculate the test statistic and p-value\n",
    "chi2, p, dof, ex = chi2_contingency(drink) # Code for calculating test statistic and p-value\n",
    "print(chi2)\n",
    "print(p)\n",
    "\n",
    "# Determine if null hypothesis is rejected or not\n",
    "if p<= 0.05: # Write appropriate if statement\n",
    "    print(\"The hypothesis that tea and coffee are drunk with the same frequency is rejected.\")\n",
    "else:\n",
    "    print(\"The hypothesis that tea and coffee are drunk with the same frequency is not rejected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbdfa19",
   "metadata": {},
   "source": [
    "__9.7 LAB: dropna() and fillna()__\n",
    "    \n",
    "Load the data set hmeq_small.csv (Nicapotato. \"Vallala, Ajay. \"HMEQ_Data.\" Kaggle, 25 Mar. 2018, www.kaggle.com/ajay1735/hmeq-data.) as a data frame.\n",
    "Create a new data frame with all the rows with missing data deleted.\n",
    "Create a second data frame with all missing data filled in with the mean value of the column.\n",
    "Find the means of the columns for both new data frames.\n",
    "Ex: Using only the first hundred rows, found in hmeq_sample.csv, the output is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hmeq =pd.read_csv('hmeq_small.csv')  # Code to read in hmeq_small.csv\n",
    "\n",
    "# Create a new data frame with the rows with missing values dropped\n",
    "df1 =hmeq.dropna()  # Code to delete rows with missing values\n",
    "\n",
    "# Create a new data frame with the missing values filled in by the mean of the column\n",
    "df2 = hmeq.fillna(hmeq.mean()) # Code to fill in missing values\n",
    "\n",
    "# Print the means of the columns for each new data frame\n",
    "print(\"Means for df1 are \", df1.mean()) # Code to find means of df1)\n",
    "\n",
    "print(\"Means for df2 are \", df2.mean()) # Code to find means of df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fbfeb1",
   "metadata": {},
   "source": [
    "__9.8 LAB: scale() and MinMaxScaler()__\n",
    "\n",
    "Load the hmeq_small.csv data set as a data frame.\n",
    "Standardize the data set as a new data frame.\n",
    "Normalize the data set as a new data frame.\n",
    "Print the means and standard deviations of both the standardized and normalized data.\n",
    "Ex: Using the first 100 rows, found in hmeq_sample.csv, the output is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d47044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "\n",
    "hmeq = pd.read_csv('hmeq_small.csv')# Read in the file hmeq_small.csv\n",
    "\n",
    "# Standardize the data\n",
    "standardized = preprocessing.scale(hmeq) # Code to standardize the data\n",
    "\n",
    "# Output the standardized data as a data frame\n",
    "df1 = pd.DataFrame(standardized,columns = ['LOAN','MORTDUE', 'VALUE', 'YOJ', 'CLAGE', 'CLNO', 'DEBTINC']) # Code to output as a data frame\n",
    "\n",
    "# Normalize the data\n",
    "normalized = preprocessing.MinMaxScaler().fit_transform(hmeq) # Code to normalize the data\n",
    "\n",
    "# Output the standardized data as a data frame\n",
    "df2 = pd.DataFrame(normalized,columns = ['LOAN','MORTDUE', 'VALUE', 'YOJ', 'CLAGE', 'CLNO', 'DEBTINC']) # Code to ouput as a data frame\n",
    "\n",
    "# Print the means and standard deviations of df1 and df2\n",
    "print(\"The means of df1 are \", df1.mean())\n",
    "print(\"The standard deviations of df1 are \", df1.std())\n",
    "print(\"The means of df2 are \",df2.mean())\n",
    "print(\"The standard deviations of df2 are \",df2.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2dab6a",
   "metadata": {},
   "source": [
    "#### 10.5 LAB: Logistic regression using logit()\n",
    "Using the csv file nbaallelo_log.csv and the logit function, construct a logistic regression model to classify whether a team will win or lose a game based on the team's elo_i score.\n",
    "\n",
    "Read in the file nbaaello_log.csv.\n",
    "The target feature will be converted from string to a binary feature by the provided code.\n",
    "Split the data into 70 percent training set and 30 percent testing set. Set random_state = 0.\n",
    "Use the logit function to construct a logistic regression model with wins as the target and elo_i as the predictor.\n",
    "Print the coefficients of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f62cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.formula.api as smf \n",
    "\n",
    "# load nbaallelo_log.csv into a dataframe\n",
    "df = pd.read_csv('nbaallelo_log.csv') # code to load csv file\n",
    "\n",
    "# Converts the feature \"game_result\" to a binary feature and adds as new column \"wins\"\n",
    "wins = df.game_result == \"W\"\n",
    "bool_val = np.multiply(wins, 1)\n",
    "wins = pd.DataFrame(bool_val, columns = [\"game_result\"])\n",
    "wins_new = wins.rename(columns = {\"game_result\": \"wins\"})\n",
    "df_final = pd.concat([df, wins_new], axis=1) \n",
    "\n",
    "\n",
    "# split the data df_final into training and test sets with a test size of 0.3 and random_state = 0\n",
    "train, test = train_test_split(df_final,test_size=.3,random_state=0) #  code to split df_final into training and test sets\n",
    "\n",
    "# construct a logistic model with wins and the target and elo_i as the predictor, using the training set\n",
    "lm = smf.logit(formula = 'wins ~ elo_i', data = train).fit() # code to construct logistic model using the logit function\n",
    "\n",
    "# print coefficients for the model\n",
    "print(lm.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b957ca9",
   "metadata": {},
   "source": [
    "#### 10.6 LAB: Evaluating logistic regression using LogisticRegression()\n",
    "Using the csv file nbaallelo_log.csv and the LogisticRegression function, construct a logistic regression model to classify whether a team will win or lose a game based on the team's elo_i score and evaluate the model.\n",
    "\n",
    "Read in the file nbaaello_log.csv.\n",
    "The target feature will be converted from string to a binary feature by the provided code.\n",
    "Split the data into 70 percent training set and 30 percent testing set. Set random_state = 0.\n",
    "Use the LogisticRegression function to construct a logistic regression model with wins as the target and elo_i as the predictor.\n",
    "Use the test set to predict the wins from the elo_i score.\n",
    "Construct and print the confusion matrix.\n",
    "Calculate and print the sensitivity.\n",
    "Calculate and print the specificity.\n",
    "Note: Use ravel() from numpy to flatten the second argument of LogisticRegression.fit() into a 1-D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8257448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# load nbaallelo_log.csv into a dataframe\n",
    "df = pd.read_csv('nbaallelo_log.csv')# code to load csv file\n",
    "\n",
    "# Converts the feature \"game_result\" to a binary feature and adds as new column \"wins\"\n",
    "wins = df.game_result == \"W\"\n",
    "bool_val = np.multiply(wins, 1)\n",
    "wins = pd.DataFrame(bool_val, columns = [\"game_result\"])\n",
    "wins_new = wins.rename(columns = {\"game_result\": \"wins\"})\n",
    "df_final = pd.concat([df, wins_new], axis=1) \n",
    "\n",
    "# split the data df_final into training and test sets with a test size of 0.3 and random_state = 0\n",
    "train, test = train_test_split(df_final, test_size=0.3, random_state=0) # code to split df_final into training and test sets\n",
    "\n",
    "# build the logistic model using the LogisticRegression function with wins as the target variable and elo_i as the predictor.\n",
    "lm = LogisticRegression()\n",
    "lm.fit(train[['elo_i']],train['wins'])\n",
    "\n",
    "# use the test set to predict the wins from the elo_i score\n",
    "predictions = lm.predict(test[['elo_i']])# code to predict wins\n",
    "\n",
    "# generate confusion matrix\n",
    "conf =  confusion_matrix(test['wins'],predictions)# code to generate confusion matrix\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "\n",
    "print(\"confusion matrix is \\n\", conf)\n",
    "\n",
    "# calculate the sensitivity\n",
    "sens = tn / (tn+fp) # code to calculate the sensitivity\n",
    "print(f'Sensitivity is {sens:.6f}')\n",
    "\n",
    "# calculate the specificity\n",
    "spec = tp / (tp + fn) # code to calculate the specificity\n",
    "print (f'Specificity is {spec:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7edb66f",
   "metadata": {},
   "source": [
    "#### 11.5 LAB: k-means clustering\n",
    "Using the csv file nbaallelo_log.csv and the KMeans function, construct a clustering model with k = 2.\n",
    "\n",
    "Read in the file nbaallelo_log.csv as df.\n",
    "Create a data frame, x, by subsetting the pts and elo_i.\n",
    "Use the KMeans function to construct a clustering model with k = 2, and pts and elo_i as the features of interest.\n",
    "Print the centroids of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a1fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    " # load nbaallelo_log.csv into a dataframe\n",
    "df = pd.read_csv('nbaallelo_log.csv')\n",
    "\n",
    "x = df[['pts', 'elo_i']]\n",
    "# subset the pts and elo_i columns\n",
    "\n",
    "kmeans = KMeans(n_clusters=2,random_state = 2 )# create the clustering model with 2 clusters, use the parameter random_state = 2 to make reproducible\n",
    "kmeans.fit_predict(x,)\n",
    "\n",
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97a876",
   "metadata": {},
   "source": [
    "#### 12.7 LAB: Using the DecisionTreeClassifier() on the iris data\n",
    "Write a program that splits a dataset into training and test set, builds a classification tree, and outputs a confusion matrix. The program should do the following:\n",
    "\n",
    "load the iris.csv dataset\n",
    "create a dataframe, x, using the petal_length and sepal_length as features\n",
    "create a dataframe, y, using species\n",
    "split the data into training and test sets with 0.25 test size and random_state = 0\n",
    "standardize x_train and x_test\n",
    "initialize the decision tree with criterion = \"gini\", random_state = 100, max_depth=3, min_samples_leaf=5\n",
    "run the decision tree on x_test\n",
    "generate the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee87cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# load the iris dataset\n",
    "iris = pd.read_csv(\"iris.csv\")\n",
    "\n",
    "x = iris.iloc[:, [1,2]].values # subset the data containing petal length and sepal length\n",
    "\n",
    "y = iris.iloc[:, 4].values# subset the data containing the labels\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)\n",
    "# splits the data into training and test sets for both x and y, with random_state = 0\n",
    "\n",
    "scaler = StandardScaler()# standardize x_train and x_test\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "cart = DecisionTreeClassifier(criterion = \"gini\", random_state =100, max_depth = 3, min_samples_leaf = 5) # initialize and run the decision tree using the following:\n",
    "# criterion = \"gini\", random_state = 100, max_depth=3, min_samples_leaf=5\n",
    "cart.fit(x_train, y_train)# fit the x_train and y_train data\n",
    "y_pred = cart.predict(x_test) # use the cart model to make predictions using x_test\n",
    "\n",
    "\n",
    "conf = metrics.confusion_matrix(y_pred, y_test) # give the confusion matrix using y_test and y_pred\n",
    "\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea98c45c",
   "metadata": {},
   "source": [
    "#### 12.8 LAB: Using the DecisionTreeClassifier() on the NBA data\n",
    "Construct a decision tree to classify whether an NBA team won or lost a game based on the team's points in the game, elo score entering the game, and win equivalence.\n",
    "\n",
    "Read nbaallelo_log.csv into a data frame.\n",
    "Subset the data containing pts, elo_i, and win_equivalent.\n",
    "Subset the data containing the labels, which are in the feature game_result.\n",
    "Standardize the data.\n",
    "Split the data into 75 % train and 25 % test sets.\n",
    "Use DecisionTreeClassifier() to initialize a classification tree using the train data.\n",
    "Use the classification tree to predict the results for the test data.\n",
    "Construct and print the confusion matrix. Ex: If only pts and elo_i are used, the output is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29765ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "df = pd.read_csv('nbaallelo_log.csv')\n",
    "X = df[['pts', 'elo_i', 'win_equiv']]\n",
    "y = df['game_result']\n",
    "X_standardized = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "test_size=0.25, random_state = 0)\n",
    "dt_classifier = DecisionTreeClassifier(criterion = \"gini\",\n",
    "random_state = 50, max_depth = 3, min_samples_leaf = 5)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "conf = confusion_matrix(y_test, y_pred)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3449f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
